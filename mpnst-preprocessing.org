* MPNST Sequencing Read Pre-processing                              :bioinfo:
** Setup
*** Repository
#+begin_src bash
mkdir -p "${repo}/workflow/scripts"            
#+end_src
*** Bash
**** Configurations
- Local (by $HOSTNAME)
  - jeff-mac-book
    #+begin_src bash :noweb yes :tangle ./config/jeff-mac-book.sh
#!/usr/bin/env bash
#########1#########2#########3#########4#########5#########6#########7#########8

#####################################################################
###   Bash configuration script for MPNST fragmentomics project   ###
#####################################################################

# Host-local variables
data_dir=/mnt/ris/aadel/mpnst
mntpt=/mnt/ris/aadel
repo=/home/jeszyman/repos/mpnst-preprocessing
sif_dir=/home/jeszyman/sing_containers
threads=8

<<bash_common_config>>
    #+end_src
  - [[file:config/jeszyman-work.sh][jeszyman-work]]
    #+begin_src bash :noweb yes :tangle ./config/jeszyman-work.sh 
#!/usr/bin/env bash
#########1#########2#########3#########4#########5#########6#########7#########8

#####################################################################
###   Bash configuration script for MPNST fragmentomics project   ###
#####################################################################

# Host-local variables
data_dir=/mnt/ris/aadel/mpnst
mntpt=/mnt/ris/aadel
repo=/home/jeszyman/repos/mpnst-preprocessing
sif_dir=/home/jeszyman/sing_containers
threads=8

<<bash_common_config>>
    #+end_src
  - [[file:config/aclm350.sh][aclm350]]
    #+begin_src bash :noweb yes :tangle ./config/aclm350.sh 
#!/usr/bin/env bash
#########1#########2#########3#########4#########5#########6#########7#########8

################################################################################
###   Bash configuration script for MPNST fragmentomics project on ACLM350   ###
################################################################################

# Host-local variables
data_dir=/mnt/ris/aadel/mpnst
mntpt=/mnt/ris/aadel
repo=/home/jszymanski/repos/mpnst-preprocessing
sif_dir=/home/jszymanski/sing_containers
threads=40

<<bash_common_config>>
    #+end_src
- Common
  #+name: bash_common_config
  #+begin_src bash :noweb yes
# Check git file hook is read-able
if [ -r "${repo}/.git/hooks/precommit" ]; then
   echo "Git size check is read-able"
else
    echo
    "Git size check is not read-able"
    exit 1
fi
          
# Check mount point  
if grep -qs $mntpt /proc/mounts; then
    echo "RIS storage mounted."
else
    echo "RIS storage NOT mounted!!!"
fi

# Check local singularity build present
if [ -r ${sif_dir}/frag.sif ]; then
    echo "Local singularity file present"
else
    echo "Local singularity file not found."
fi

# Check if local singularity container is present and up-to-date
if [ -r "${HOME}/sing_containers/biotools.sif" ]; then
    echo "Local biotools singularity container exists"
else
    echo "No local biotools singularity container, attempting to fetch..."
    mkdir -p "${HOME}/sing_containers"
    cp /mnt/ris/aadel/jeszyman/sing_containers/biotools.sif "${HOME}/sing_containers"
fi 

if [ ${HOME}/sing_containers/biotools.sif -ot /mnt/ris/aadel/jeszyman/sing_containers/biotools.sif ];
then
    echo "Local biotools container out of date, updating..."
    cp /mnt/ris/aadel/jeszyman/sing_containers/biotools.sif "${HOME}/sing_containers"
else
    echo "Local biotools singularity container is up to date"
fi

sing_biotools() {
    singularit shell --bind /mnt:/mnt ~/sing_containers/biotools.sif            
}

launch_frag() { 
    if [ -f /.dockerenv ]; then
        echo "shell already in docker, exiting";
        exit 1;
    else
        docker run --env HOME=${HOME} --hostname ${HOSTNAME} --interactive --tty --volume /home/:/home/ --volume /tmp/:/tmp/ --volume /mnt/:/mnt/ --user $(id -u ${USER}) -w "$repo" jeszyman/frag /bin/bash;
    fi
}
#+end_src           

*** Git and github
#+begin_src bash
repo=/home/jeszyman/repos/mpnst-preprocessing
cd $repo

git init
git add -A 
git commit -m "first commit"

git remote add origin git@github.com:jeszyman/mpnst-preprocessing.git
git branch -M master
git push -u origin master
#+end_src


*** Docker and singularity - none; use biotools submodule
#+begin_src bash :tangle ./src/launch_singularity_shell.sh
singularity shell --bind /mnt:/mnt ~/sing_containers/biotools.sif            
#+end_src


*** Snakemake
**** Configuration YAMLs
- consider a workdir: "/output/dir" 
- [[file:config/aclm350.yaml][ACLM350]]
  #+begin_src bash :tangle config/aclm350.yaml
container: "/home/jszymanski/sing_containers/frag.sif"
data_dir: "/mnt/ris/aadel/mpnst"
threads: 40
repo: "/drive3/users/jszymanski/repos/mpnst-preprocessing"
cap_extract_script: "/drive3/users/jszymanski/repos/mpnst-preprocessing/workflow/scripts/cp_fastq_extract_auto.pl"
#+end_src
- [[file:config/common.yaml][common]]
  #+begin_src bash :tangle config/common.yaml
cap_extract_script: "src/cp_fastq_extract_auto.pl"
inputs_dir: "/mnt/ris/aadel/mpnst/inputs"
demultiplex_dir: "/mnt/ris/aadel/mpnst/cappseq/demultiplexed"
hg38_fasta: "GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai"
hg38_fasta_ftp: "https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai"
hg38_bwa_index_ftp: "https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bwa_index.tar.gz"
hg38_bwa_index_zip: "GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bwa_index.tar.gz"
ref_dir: "/mnt/ris/aadel/mpnst/ref"
log_dir: "/mnt/ris/aadel/mnpst/logs"
#+end_src
- repo_test
  #+begin_src bash :tangle ./config/repo_test.yaml
container: "/home/jeszyman/sing_containers/biotools.sif"        
raw_fq_dir: "test/fastq"
qc_dir: "test/qc"
log_dir: "test/logs"
threads: 4
inputs_dir: "test/inputs"
processed_fq_dir: "test/processed-fastq"
unpr_fq_dir: "test/unpaired-fastq"
bwa_index: "test/ref/chr8"
bam_dir: "test/bam"
rename_dir: "test/cappseq_fastq"
extracted_dir: "test/extracted_fastq"
fq_symlink_dir: "test/symlink-fastq"
MILREADS:
  - "10"
  - "20"
#+end_src
**** Run commands
:PROPERTIES:
:header-args: :tangle no
:END:
#+begin_src bash :tangle ./src/smk_draw.sh
eval "$(command conda 'shell.bash' 'hook' 2> /dev/null)"

conda activate snakemake

snakemake \
    --configfile config/repo_test.yaml \
    --cores $threads \
    --rulegraph \
    --snakefile ./workflow/read_preprocess.smk | dot -Tpdf > resources/read_preprocess_dagtmp/test.pdf
#+end_src

#+begin_src bash :tangle ./src/smk_repo_test.sh
eval "$(command conda 'shell.bash' 'hook' 2> /dev/null)"

conda activate snakemake

output_dirs=( "bam" "processed-fastq" "qc" "symlink-fastq" "unpaired-fastq" )

for dir in ${output_dirs[@]};
do
               if [ -d test/${dir} ]; then \rm -rf test/${dir}; fi
done

snakemake \
    --configfile config/repo_test.yaml \
    --cores $threads \
    --rerun-incomplete \
    --use-singularity \
    --forceall \
    --snakefile ./workflow/read_preprocess.smk
#+end_src
#+begin_src bash :tangle ./src/smk_run.sh
#!/bin/bash
#########1#########2#########3#########4#########5#########6#########7#########8
eval "$(command conda 'shell.bash' 'hook' 2> /dev/null)"
####################################
###   Choose and Run Snakefile   ###
####################################

# Setup
#set -euxov pipefail

source config/${HOSTNAME}.sh
echo "The following `*.smk` archives were found; select one:"

# set the prompt used by select, replacing "#?"
PS3="Use number to select an option"

select filename in ./workflow/*.smk

do
    if [[ "$filename" == "" ]]
    then
        echo "'$REPLY' is not a valid number"
        continue
    fi
    echo $filename
    select run_option in dry_run normal force_final force_all
    do
        echo selected $run_option
        case $run_option in
            dry_run)
                conda activate snakemake
                snakemake \
                    --configfile config/${HOSTNAME}.yaml \
                    --cores $threads \
                    --directory ${repo} \
                    --dry-run \
                    --rerun-incomplete \
                    --singularity-args "--bind $mntpt:$mntpt" \
                    --use-singularity \
                    --snakefile $filename
                ;;
            normal) 
                conda activate snakemake
                select nohup_option in no yes
                do
                    case $nohup_option in
                        no)
                            snakemake \
                                --configfile config/${HOSTNAME}.yaml \
                                --cores $threads \
                                --directory ${repo} \
                                --singularity-args "--bind $mntpt:$mntpt" \
                                --use-singularity \
                                --snakefile $filename
                            ;;
                        yes)
                            nohup snakemake \
                                  --configfile config/${HOSTNAME}.yaml \
                                  --cores $threads \
                                  --directory ${repo} \
                                  --singularity-args "--bind $mntpt:$mntpt" \
                                  --use-singularity \
                                  --snakefile $filename
                            ;;
                    esac
                done                
                ;;
            force_final)
                conda activate snakemake
                select nohup_option in no yes
                do
                    case $nohup_option in
                        no)                                          
                            snakemake \
                                --configfile config/${HOSTNAME}.yaml \
                                --cores $threads \
                                --directory ${repo} \
                                --force \
                                --singularity-args "--bind $mntpt:$mntpt" \
                                --use-singularity \
                                --snakefile $filename
                            ;;
                        yes)
                            nohup snakemake \
                                  --configfile config/${HOSTNAME}.yaml \
                                  --cores $threads \
                                  --directory ${repo} \
                                  --force \
                                  --singularity-args "--bind $mntpt:$mntpt" \
                                  --use-singularity \
                                  --snakefile $filename
                            ;;
                    esac
                done                
                ;;            
            force_all)
                conda activate snakemake
                select nohup_option in no yes
                do
                    case $nohup_option in
                        no)                                          
                
                snakemake \
                    --configfile config/${HOSTNAME}.yaml \
                    --cores $threads \
                    --directory ${repo} \
                    -F \
                    --singularity-args "--bind $mntpt:$mntpt" \
                    --use-singularity \
                    --snakefile $filename
                                            ;;
                        yes)
                nohup snakemake \
                    --configfile config/${HOSTNAME}.yaml \
                    --cores $threads \
                    --directory ${repo} \
                    -F \
                    --singularity-args "--bind $mntpt:$mntpt" \
                    --use-singularity \
                    --snakefile $filename
                            ;;
                    esac
                done                
                ;;            
        esac
        break
    done
    break
done
#+end_src

#+begin_src bash :tangle ./src/smk_test.sh
#!/bin/bash

eval "$(command conda 'shell.bash' 'hook' 2> /dev/null)"

echo "The following `*.smk` archives were found; select one:"

# set the prompt used by select, replacing "#?"
PS3="Use number to select an option"

select filename in ./workflow/*.smk

do
    if [[ "$filename" == "" ]]
    then
        echo "'$REPLY' is not a valid number"
        continue
    fi
    conda activate snakemake
    snakemake --dry-run --snakefile $filename \
              --configfile config/${HOSTNAME}.yaml \
              --cores $threads \
              --directory ${repo} \
              --rerun-incomplete \
              --singularity-args "--bind $mntpt:$mntpt" \
              --use-singularity 
done
#+end_src

#+begin_src bash :tangle ./src/smk_run.sh
#!/bin/bash
#########1#########2#########3#########4#########5#########6#########7#########8

####################################
###   Choose and Run Snakefile   ###
####################################

# Setup
#set -euxov pipefail
source config/${HOSTNAME}.sh
echo "The following `*.smk` archives were found; select one:"

# set the prompt used by select, replacing "#?"
PS3="Use number to select an option"

select filename in ./workflow/*.smk

do
    if [[ "$filename" == "" ]]
    then
        echo "'$REPLY' is not a valid number"
        continue
    fi
    echo $filename
    select run_option in dry_run normal force_final force_all
    do
        echo selected $run_option
        case $run_option in
            dry_run)
                source activate snakemake
                snakemake \
                    --configfile config/${HOSTNAME}.yaml \
                    --cores $threads \
                    --directory ${repo} \
                    --dry-run \
                    --rerun-incomplete \
                    --singularity-args "--bind $mntpt:$mntpt" \
                    --use-singularity \
                    --snakefile $filename
                ;;
            normal) 
                source activate snakemake
                select nohup_option in no yes
                do
                    case $nohup_option in
                        no)
                            snakemake \
                                --configfile config/${HOSTNAME}.yaml \
                                --cores $threads \
                                --directory ${repo} \
                                --singularity-args "--bind $mntpt:$mntpt" \
                                --use-singularity \
                                --snakefile $filename
                            ;;
                        yes)
                            nohup snakemake \
                                  --configfile config/${HOSTNAME}.yaml \
                                  --cores $threads \
                                  --directory ${repo} \
                                  --singularity-args "--bind $mntpt:$mntpt" \
                                  --use-singularity \
                                  --snakefile $filename
                            ;;
                    esac
                done                
                ;;
            force_final)
                source activate snakemake
                select nohup_option in no yes
                do
                    case $nohup_option in
                        no)                                          
                            snakemake \
                                --configfile config/${HOSTNAME}.yaml \
                                --cores $threads \
                                --directory ${repo} \
                                --force \
                                --singularity-args "--bind $mntpt:$mntpt" \
                                --use-singularity \
                                --snakefile $filename
                            ;;
                        yes)
                            nohup snakemake \
                                  --configfile config/${HOSTNAME}.yaml \
                                  --cores $threads \
                                  --directory ${repo} \
                                  --force \
                                  --singularity-args "--bind $mntpt:$mntpt" \
                                  --use-singularity \
                                  --snakefile $filename
                            ;;
                    esac
                done                
                ;;            
            force_all)
                source activate snakemake
                select nohup_option in no yes
                do
                    case $nohup_option in
                        no)                                          
                
                snakemake \
                    --configfile config/${HOSTNAME}.yaml \
                    --cores $threads \
                    --directory ${repo} \
                    -F \
                    --singularity-args "--bind $mntpt:$mntpt" \
                    --use-singularity \
                    --snakefile $filename
                                            ;;
                        yes)
                nohup snakemake \
                    --configfile config/${HOSTNAME}.yaml \
                    --cores $threads \
                    --directory ${repo} \
                    -F \
                    --singularity-args "--bind $mntpt:$mntpt" \
                    --use-singularity \
                    --snakefile $filename
                            ;;
                    esac
                done                
                ;;            
        esac
        break
    done
    break
done
#+end_src

#+begin_src bash
#cd ~/repos/mpnst
conda activate snakemake
source config/"${HOSTNAME}.sh"                                                   

nohup snakemake \
  --configfile config/${HOSTNAME}.yaml \
  --directory "${repo}" \
  --cores 10 \
  --printshellcmds \
  --singularity-args "--bind $mntpt:$mntpt" \
  --snakefile workflows/cappseq.smk \
  --use-singularity 


nohup snakemake \
    --configfile config/${HOSTNAME}.yaml \
    --cores $threads \
    --directory "${repo}" \
    --printshellcmds \
    --singularity-args "--bind $mntpt:$mntpt" \
    --snakefile workflow/frag.smk \
    --use-singularity 


nohup snakemake \
  --configfile config/${HOSTNAME}.yaml \
  --cores $threads \
  --directory "${repo}" \
  --printshellcmds \
  --singularity-args "--bind $mntpt:$mntpt" \
  --snakefile workflow/frag.smk \
  --use-singularity \
  --rerun-incomplete
#+end_src

#+begin_src bash
#cd ~/repos/mpnst
conda activate snakemake
source config/"${HOSTNAME}.sh"

snakemake \
  --configfile config/${HOSTNAME}.yaml \
  --cores $threads \
  --directory "${repo}" \
  --dry-run \
  --printshellcmds \
  --singularity-args "--bind $mntpt:$mntpt" \
  --snakefile workflow/frag.smk \
  --use-singularity 

snakemake \
  --configfile config/${HOSTNAME}.yaml \
  --cores $threads \
  --directory "${repo}" \
  --printshellcmds \
  --singularity-args "--bind $mntpt:$mntpt" \
  --snakefile workflow/frag.smk \
  --use-singularity 


snakemake \
  --configfile config/${HOSTNAME}.yaml \
  --cores $threads \
  --directory "${repo}" \
  --printshellcmds \
  --singularity-args "--bind $mntpt:$mntpt" \
  --snakefile workflow/frag.smk \
  --use-singularity \
  --rulegraph | dot -Tpdf > $repo/resources/frag_rules.pdf
#+end_src


** TODO Sequence pre-processing, alignment, and quality control 
*** Integration testing setup
#+begin_src bash :tangle ./src/seq_preprocess_integration_setup.sh
#!/bin/echo Run:.

# For documentation, not intended to be executable 

if [ -d test ]; then \rm -rf test; fi
mkdir -p test/fastq
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/19_2_082_R1.fastq.gz | head -n 100000 > "test/fastq/mpnst1_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/19_2_082_R2.fastq.gz | head -n 100000 > "test/fastq/mpnst1_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/25_2_072_R1.fastq.gz | head -n 100000 > "test/fastq/mpnst2_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/25_2_072_R2.fastq.gz | head -n 100000 > "test/fastq/mpnst2_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/37_JS0050CD112717_R1.fastq.gz | head -n 100000 > "test/fastq/plex1_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/37_JS0050CD112717_R2.fastq.gz | head -n 100000 > "test/fastq/plex1_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/30_JS0044CD112818_R1.fastq.gz | head -n 100000 > "test/fastq/plex2_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/30_JS0044CD112818_R2.fastq.gz | head -n 100000 > "test/fastq/plex2_R2.fastq"
for file in "test/fastq/*.fastq"; do gzip $file; done

mkdir -p "test/inputs"
wget --directory-prefix="test/inputs/" https://raw.githubusercontent.com/usadellab/Trimmomatic/main/adapters/TruSeq3-PE.fa
wget --directory-prefix="test/inputs/" https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz

cp resources/samples.tsv test/inputs/

mkdir -p test/ref
zcat "test/inputs/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz" | grep -A 2000 chr8 > test/inputs/chr8.fa
\rm test/inputs/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz

singularity shell ~/sing_containers/biotools.sif
bwa index -p test/ref/chr8 test/inputs/chr8.fa
exit
#+end_src
*** [[file:./workflow/read_qc.smk][Snakefile]]                                                           :smk:
:PROPERTIES:
:header-args:snakemake: :tangle ./workflow/read_preprocess.smk
:END:              
**** Smk preamble
#+begin_src snakemake
container: config["container"]
import pandas as pd
import numpy as np

samples = pd.read_table(config["inputs_dir"] + "/samples.tsv")
sampledict = dict(zip(samples['new_name'], samples['old_name']))

wildcard_constraints:
    read_id='|'.join([re.escape(x) for x in sampledict.keys()]),
    
#+end_src              
**** Smk rules
***** All rule
#+begin_src snakemake
rule all:
    input:
        expand(config["processed_fq_dir"] + "/{read_id}_proc_{read}.fastq.gz", read_id = sampledict.keys(), read = ["R1","R2"]),
        expand(config["unpr_fq_dir"] + "/{read_id}_unpr_R1.fastq.gz", read_id = sampledict.keys(), read = ["R1","R2"]),
	expand(config["bam_dir"] + "/{read_id}_dedup.bam", read_id = sampledict.keys()),
        config["qc_dir"] + "/all_qc.html",
        expand(config["bam_dir"] + "/{read_id}_ds{milreads}.bam", read_id = sampledict.keys(), milreads = config["MILREADS"]),
#+end_src                            
***** Rename :smk_rule:
- Snakemake
#+begin_src snakemake
rule rename:
    params:
        old_sample_id=lambda wcs: sampledict[wcs.f],
    output:
        read1=config["fq_symlink_dir"] + "/{f}_R1.fastq.gz",
        read2=config["fq_symlink_dir"] + "/{f}_R2.fastq.gz",	
    shell:
        """
        if [ -f {output.read1} ]; then \\rm {output.read1}; fi
        if [ -f {output.read2} ]; then \\rm {output.read2}; fi
        ln -s --relative "{config[raw_fq_dir]}/{params.old_sample_id}_R1.fastq.gz" {output.read1}
        ln -s --relative "{config[raw_fq_dir]}/{params.old_sample_id}_R2.fastq.gz" {output.read2}
        """
#+end_src

***** Read pre-processing                                          :smk_rule:
- Snakemake
  #+begin_src snakemake
rule trimmomatic:
    input:
        read1 = config["fq_symlink_dir"] + "/{fq_id}_R1.fastq.gz",
        read2 = config["fq_symlink_dir"] + "/{fq_id}_R2.fastq.gz",
    params:
        adapter_fasta = config["inputs_dir"] + "/TruSeq3-PE.fa",
    output:
        read1 = config["processed_fq_dir"] + "/{fq_id}_proc_R1.fastq.gz",
        read1_unpr = config["unpr_fq_dir"] + "/{fq_id}_unpr_R1.fastq.gz",
        read2 = config["processed_fq_dir"] + "/{fq_id}_proc_R2.fastq.gz",
        read2_unpr = config["unpr_fq_dir"] + "/{fq_id}_unpr_R2.fastq.gz",	
    log:
        int = config["log_dir"] + "/trimmomatic_trimlog_{fq_id}.log",
        main = config["log_dir"] + "/trimmomatic_{fq_id}.log",
    shell:
        """
        trimmomatic PE \
                    -threads {config[threads]} \
                    -trimlog {log.int} \
                    {input.read1} {input.read2} \
                    {output.read1} {output.read1_unpr} \
                    {output.read2} {output.read2_unpr} \
                    ILLUMINACLIP:{params.adapter_fasta}:2:30:10 \
                    LEADING:10 TRAILING:10 MAXINFO:50:0.97 MINLEN:20 &> {log.main}
        """
#+end_src
- Reference
  - Trimmomatic parameters based on Taylor's parameters ([[https://mail.google.com/mail/u/0/#search/sundby+fastq/FMfcgzGmvLWSbsmhDsffvSSWfjWdQhhR?projector=1&messagePartId=0.1][email]])
  - https://github.com/AAFC-BICoE/snakemake-trimmomatic/blob/master/Snakefile
***** Alignment 
- Snakemake
  #+begin_src snakemake
rule align:
    input:
        read1 = config["processed_fq_dir"] + "/{fq_id}_proc_R1.fastq.gz",
        read2 = config["processed_fq_dir"] + "/{fq_id}_proc_R2.fastq.gz",
    output:
        config["bam_dir"] + "/{fq_id}.sam",
    log:
        config["log_dir"] + "/align_{fq_id}.log"
    shell:
        """
        bwa mem -M -t 4 {config[bwa_index]} {input.read1} {input.read2} > {output}
	"""
#+end_src

***** Alignment processing
- Snakemake
  #+begin_src snakemake
rule alignment_processing:
    input:
        config["bam_dir"] + "/{fq_id}.sam",
    output:
        bam = config["bam_dir"] + "/{fq_id}_raw.bam",
        dedup = temp(config["bam_dir"] + "/{fq_id}_dedup_unsort.bam"),
        sort = config["bam_dir"] + "/{fq_id}_dedup.bam",
        index = config["bam_dir"] + "/{fq_id}_dedup.bam.bai",
    log:
        config["log_dir"] + "/alignment_processing_{fq_id}.log"
    shell:
        """
        sambamba view -t {config[threads]} -S -f bam {input} > {output.bam}
        sambamba markdup -r -t {config[threads]} {output.bam} {output.dedup}
        sambamba sort -t {config[threads]} {output.dedup} -o {output.sort}
        sambamba index -t {config[threads]} {output.sort}
        """
#+end_src
***** FastQC                                                       :smk_rule:
- Snakemake
  #+begin_src snakemake
rule fastqc:
    input: 
        raw=config["fq_symlink_dir"] + "/{read_id}_{read}.fastq.gz",
	proc=config["processed_fq_dir"] + "/{read_id}_proc_{read}.fastq.gz",
    params: 
        out_dir = config["qc_dir"],
    output:
        raw_html = config["qc_dir"] + "/{read_id}_{read}_fastqc.html",
        proc_html = config["qc_dir"] + "/{read_id}_proc_{read}_fastqc.html", 	
    log: 
        raw = config["log_dir"] + "/fastqc_raw_{read_id}_{read}.log",
        proc = config["log_dir"] + "/fastqc_proc_{read_id}_{read}.log",	
    shell:
        """
	fastqc --outdir {params.out_dir} \
	--quiet \
	--threads {config[threads]} {input.raw} &> {log}
	fastqc --outdir {params.out_dir} \
	--quiet \
	--threads {config[threads]} {input.proc} &> {log}
        """
#+end_src
***** Alignment QC
#+begin_src snakemake
rule alignment_qc:
    input:
        config["bam_dir"] + "/{fq_id}_{bam_step}.bam",
    output:
        samstat = config["qc_dir"] + "/{fq_id}_{bam_step}_samstats.txt",
        flagstat = config["qc_dir"] + "/{fq_id}_{bam_step}_flagstat.txt",        
    shell:
        """
        samtools stats {input} > {output.samstat}
        samtools flagstat {input} > {output.flagstat}
        """
#+end_src
****** TODO Downsample Bams
#+name: downsample_bam
#+begin_src bash 
function downsample_bam {

## Calculate the sampling factor based on the intended number of reads:
FACTOR=$(samtools idxstats $1 | cut -f3 | awk -v COUNT=$2 'BEGIN {total=0} {total += $1} END {print COUNT/total}')

if [[ $FACTOR > 1 ]]; then 
    echo "DS reads exceeds total for $1"
else
    sambamba view -s $FACTOR -f bam -l 5 $1    
fi
}

#+end_src

#+name: downsample_bam
#+begin_src bash :tangle ./src/functions.sh
function downsample_bam {

## Calculate the sampling factor based on the intended number of reads:
FACTOR=$(samtools idxstats $1 | cut -f3 | awk -v COUNT=$2 'BEGIN {total=0} {total += $1} END {print COUNT/total}')

if [[ $FACTOR > 1 ]]; then 
    echo "DS reads exceeds total for $1"
else
    sambamba view -s $FACTOR -f bam -l 5 $1    
fi
}

#+end_src

***** Multiqc         
#+begin_src snakemake
rule multiqc:
    input:
        expand(config["qc_dir"] + "/{read_id}_{read}_fastqc.html", read_id = sampledict.keys(), read = ["R1","R2"]),
        expand(config["qc_dir"] + "/{read_id}_proc_{read}_fastqc.html", read_id = sampledict.keys(), read = ["R1","R2"]),
        expand(config["qc_dir"] + "/{read_id}_{bam_step}_samstats.txt", read_id = sampledict.keys(), bam_step = ["raw","dedup"]),
        expand(config["qc_dir"] + "/{read_id}_{bam_step}_flagstat.txt", read_id = sampledict.keys(), bam_step = ["raw","dedup"]),
    params:
        out_dir = config["qc_dir"]
    output:
        config["qc_dir"] + "/all_qc.html"
    shell:
        """
        multiqc {params.out_dir} \
        --force \
        --outdir {params.out_dir} \
        --filename all_qc 
        """
#+end_src
***** Downsample bams
#+begin_src snakemake
rule downsample_bams:
    input:
        bam = config["bam_dir"] + "/{fq_id}_dedup.bam",
    output:
        config["bam_dir"] + "/{fq_id}_ds{milreads}.bam",
    shell:
        """
        reads=$(echo {wildcards.milreads}000000)
        workflow/scripts/downsample_bam.sh {input} $reads {output}
        """
#+end_src

#+begin_src bash :tangle ./workflow/scripts/downsample_bam.sh
## Calculate the sampling factor based on the intended number of reads:
FACTOR=$(samtools idxstats $1 | cut -f3 | awk -v COUNT=$2 'BEGIN {total=0} {total += $1} END {print COUNT/total}')

if [[ $FACTOR > 1 ]]; then 
    echo "DS reads exceeds total for $1"
    cp $1 $3
else
    sambamba view -s $FACTOR -f bam -l 5 $1 > $3
fi
#+end_src

**** Dev
:PROPERTIES:
:header-args:snakemake: :tangle no
:END:
***** TODO Library QC Dataframe
#+begin_src R
library(tidyverse)

flagstat_raw = as_tibble(read.table("/home/jeszyman/repos/mpnst-preprocessing/test/qc/all_qc_data/multiqc_samtools_flagstat.txt", header = T, sep = '\t'))

flagstat_mod =
  flagstat_raw %>%
  mutate(library_id = substr(Sample, 1, 6)) %>%
  mutate(bam_type = gsub("_.*$","", gsub("^.......","",Sample))) %>%
  pivot_wider(names_from = bam_type, values_from = -c(library_id, bam_type), everything()) %>%
  select(library_id, everything(), -starts_with("Sample"))

samstats_raw = as_tibble(read.table("/home/jeszyman/repos/mpnst-preprocessing/test/qc/all_qc_data/multiqc_samtools_stats.txt", header = T, sep = '\t'))

samstats_mod =
  samstats_raw %>%
  mutate(library_id = substr(Sample, 1, 6)) %>%
  mutate(bam_type = gsub("_.*$","", gsub("^.......","",Sample))) %>%
  pivot_wider(names_from = bam_type, values_from = -c(library_id, bam_type), everything()) %>%
  select(library_id, everything(), -starts_with("Sample"))  
samstats_mod

#+end_src
*** Ideas
:PROPERTIES:
:header-args:snakemake: :tangle no
:END:
- frag size
  #+name: fragment_size.sh
#+begin_src bash 
#########1#########2#########3#########4#########5#########6#########7#########8
#
source ./src/setup.sh
docker_interactive
jeszyman
biotools
source ~/repos/mpnst/src/setup.sh

# Function
mpnst_fragsize() {
    bamPEFragmentSize --bamfiles $1 \
                      --numberOfProcessors $2 \
                      --binSize $3 \
                      --distanceBetweenBins $4 \
                      --outRawFragmentLengths $5
}

##
## Local variables
processors=40
bin_size=10000000
distance_between_bins=10000000
min_bam_size=100000000

#
# Generate bam file lists
#  Note: Small or empty bams kill bamPEFragmentSize and must be excluded
##
## For fragment-filtered bams
declare -a frag_filt_bam=()
for file in $localdata/frag-filt-bams/*.dedup.sorted.frag.sorted.bam;
do
    bamsize=$(wc -c <"$file")
    if [ $bamsize -ge $min_bam_size ]; then
        frag_filt_bam+=("$file")
    fi
done
##
## For deduped full bams
declare -a dedup_bam
for file in $localdata/bams/*.dedup.sorted.bam;
do
    bamsize=$(wc -c <"$file")
    if [ $bamsize -ge $min_bam_size ]; then
        dedup_bam+=("$file")
    fi
done
##
mkdir -p $localdata/frag_size
#
for file in "${frag_filt_bam[@]}";
do
    base=`basename $file`
    if [ $localdata/frag_size/${base}.fragsize.tsv -ot $file ]; then    
        mpnst_fragsize \
            $file \
            $processors \
            $bin_size \
            $distance_between_bins \
            $localdata/frag_size/${base}.fragsize.tsv
    fi
done
#
for file in "${dedup_bam[@]}";
do
    base=`basename $file`
    if [ $localdata/frag_size/${base}.fragsize.tsv -ot $file ]; then
        mpnst_fragsize \
            $file \
            $processors \
            $bin_size \
            $distance_between_bins \
            $localdata/frag_size/${base}.fragsize.tsv
    fi
done
#
rm $localdata/frag_size/frag_size_summary.tsv
touch $localdata/frag_size/frag_size_summary.tsv
for file in $localdata/frag_size/*.fragsize.tsv; do
    cat $file | tail -n +3 >> $localdata/frag_size/frag_size_summary.tsv
done
#
sed -i '1 i\size\toccurences\tsample' $localdata/frag_size/frag_size_summary.tsv
#
rm $repo/data/frag_size_summary.tsv
rm $repo/data/frag_size_summary_too_big
#
summary_file_size=$(wc -c <"$localdata/frag_size/frag_size_summary.tsv")
max_size=1000000
if [ $summary_file_size -gt $max_size ]; then
    touch $repo/data/frag_size_summary_too_big
else
    cp $localdata/frag_size/frag_size_summary.tsv $repo/data/frag_size_summary.tsv
fi
#
exit
#+end_src
- Fragment size 
  #+name: fragment-sampling
  #+begin_src bash 
#
# Samples fragment size by TLEN in bam files
#
# Setup
exit
source ~/repos/mpnst/bin/local-setup.sh
## Variables
fragsampledir=$localdata/tmp
## Directories
rm -rf $fragsampledir
mkdir -p $fragsampledir
#
# Get lists of bam files to sample
find /localdata/box/NCI FASTQ/ -name 
find /duo4/.mpnst/bam-nci/ -name "*.dedup.bam" > $fragsampledir/nci-invivo-bams
find /duo4/.mpnst/bam-nci/ -name "*.filt.sorted.bam" > $fragsampledir/nci-insilico-bams
#TODO ADD WASHU find /duo4/mpnst/

# TODO
## paramaterize sampleing count
#
# Run Setup
#
# Processes
## 
#
mapfile -t nci_insilico_bams < $fragsampledir/nci-insilico-bams
for file in "${nci_insilico_bams[@]}"; do
    prebase=`basename $file`
    base="${prebase%%.*}"
    sambamba view -f sam -t 30 $file | shuf --head-count 10000 > $fragsampledir/${base}_nci_insilico_sample
done
#
#########1#########2#########3#########4#########5#########6#########7#########8
mapfile -t nci_invivo_bams < $fragsampledir/nci-invivo-bams
for file in "${nci_invivo_bams[@]}"; do
    prebase=`basename $file`
    base="${prebase%%.*}"
    sambamba view -f sam -t 30 $file | shuf --head-count 10000 > $fragsampledir/${base}_nci_invivo_sample
done

cd $fragsampledir
rm frag_concat.txt
for file in $fragsampledir/*_sample; do
    awk '{ print sqrt($9^2) "_" FILENAME }' $file >> frag_concat.txt
done
sed -i '1s/^/fragsize_\n/' frag_concat.txt
>>>>>>> 2d6bf2d62424a76f5893600fce7444a867784228

sed -i -e 's/_/,/g' frag_sum_test.txt



# find /duo4/.mpnst/fastq-washu/ -name "*HiSeqW31*R1_001_TAGC*.fastq.gz" | cut -d "_" -f 1-5
#      | parallel perl ~/repos/mpnst/bin/cp-fastq-extract-auto.pl {}\_R1_001_TAGC.fastq.gz {}\_R2_001_TAGC.fastq.gz -j 24

#+end_src

#+begin_src bash
source ./src/setup.sh
docker_interactive
jeszyman
biotools
source ~/repos/mpnst/src/setup.sh
source ~/repos/mpnst/src/functions.sh

for file in $dataDIR/bam/lib*_sub20m.bam;
do
    base=$(basename -s .bam $file)
    if [ $file -nt $dataDIR/bam/${base}_frag90_150_sorted.bam ];
    then
        frag_filter $file \
                    $dataDIR/bam \
                    90 \
                    150 \
                    40
    fi    
done
#+end_src

- deeptools https://multiqc.info/docs/
- using mosdepth
  #+name: mosdepth
  #+begin_src bash 
#########1#########2#########3#########4#########5#########6#########7#########8
#
### mosdepth for WGS depth calc  ###
#
# Setup 
##

# Mosdepth per bam dir
##
## For deduped bams
for file in $localdata/bams/*.dedup.sorted.bam; do
    mosdepth_mpnst $file $localdata/bam-qc/dedup 250000000
done
##
#
# get simple tsv and send to repo

for file in $localdata/bam-qc/dedup/lib*.regions.bed.gz; do
    base=`basename -s .dedup.sorted.regions.bed.gz $file`
    zcat $file | awk -v FS='\t' -v var=$base 'NR <=24 {print var,$1,$4}' >> $localdata/bam-qc/dedup/all_dedup_coverage
done

header=library_id\\tchr\\tmean_coverage
sed -i "1 i$header" $localdata/bam-qc/dedup/all_dedup_coverage

## Local
>>>>>>> 2d6bf2d62424a76f5893600fce7444a867784228
source ~/repos/mpnst/bin/local-setup.sh
docker_interactive
biotools
##
## Functions
###
### Convert bams to wigs
bam_to_wig() {
    printf "Variables are: 1=bam_file 2=bam_suffix 3=outdir\n"
        base=`basename -s ${2} $1`        
        if [ $3/${base}.wig -ot $1 ]; then
            /opt/hmmcopy_utils/bin/readCounter --window 1000000 --quality 20 \
                                               --chromosome "chr1,chr2,chr3,chr4,chr5,chr6,chr7,chr8,chr9,chr10,chr11,chr12,chr13,chr14,chr15,chr16,chr17,chr18,chr19,chr20,chr21,chr22,chrX,chrY" $1 > $3/${base}.wig
        fi
}
###
### Run ichor for low TF 
ichor_lowfract() {
    base=`basename -s .wig $1`
    if [ $2/$base.RData -ot $1 ]; then  
        Rscript /opt/ichorCNA/scripts/runIchorCNA.R \
                --id $base \
                --WIG $1 \
                --gcWig /opt/ichorCNA/inst/extdata/gc_hg19_1000kb.wig \
                --normal "c(0.95, 0.99, 0.995, 0.999)" \
                --ploidy "c(2)" \
                --maxCN 3 \
                --estimateScPrevalence FALSE \
                --scStates "c()" \
                --outDir $2
    fi
}
##
##
mkdir -p $localdata/wigs
mkdir -p $localdata/ichor
#
# Make wigs
#
#bam_to_wig /mnt/xt3/mpnst/frag-filt-bams/lib109.dedup.sorted.frag90_150.sorted.bam .dedup.sorted.frag90_150.sorted.bam $localdata/wigs
##
for file in $localdata/frag-filt-bams/lib109*.bam; do
    bam_to_wig $file \
               .dedup.sorted.frag.sorted.bam \
               $localdata/wigs
done

## For fraction-filtered WGS cfDNA
for file in $localdata/frag-filt-bams/*.bam; do
    bam_to_wig $file \
               .dedup.sorted.frag.sorted.bam \
               $localdata/wigs
done
##
## For tumor and leukocyte WGS libraries
### Make array of genomic library file paths
genomic=($(cat /drive3/users/jszymanski/repos/mpnst/data/libraries.csv | grep -e tumor -e leukocyte | grep -v "wes" | awk -F, '{print $1}' | sed 's/"//g' | sed 's/$/.dedup.sorted.bam/g' | sed 's/^/\/mnt\/xt3\/mpnst\/bams\//g'))
###
for file in ${genomic[@]}; do
    bam_to_wig $file \
               .dedup.sorted.bam \
               $localdata/wigs
done
#
##
## Send successful file list to repo 
rm /drive3/users/jszymanski/repos/mpnst/data/wigs.tsv
for file in $localdata/wigs/*.wig;
do
    base=`basename -s .wig $file`
    echo $base >> /drive3/users/jszymanski/repos/mpnst/data/wigs.tsv
done
#
##RESUME HERE
# ichor
##
for file in $localdata/wigs/lib109*.wig; do
    ichor_lowfract $file $localdata/ichor
done


header=library_id\\tchr\\tmean_coverage
sed -i "1 i$header" $localdata/bam-qc/dedup/all_dedup_coverage

max_file_size=5000000
file_size=$(
    wc -c <"$localdata/bam-qc/dedup/all_dedup_coverage"
         )

if [ $filesize -gt $max_file_size ]; then
    touch $repo/data/qc/all_dedup_coverage_too_big
else
    cp $localdata/bam-qc/dedup/all_dedup_coverage $repo/qc/all_dedup_coverage.tsv
fi
#
#+end_src
  - Cant calcualte depths off [[file:~/repos/mpnst/data/bam_qc_data/mqc_mosdepth-coverage-per-contig_1.txt]] , d/n allow values under 1
  - [ ] for coverage, should intersect down to autosomes 
- run and extract mosdepth 
  mosdepthRAW = as_tibble(read.table(file.path(repo,"data/all_dedup_coverage.tsv"), header = T, sep = '\t', fill = TRUE))
- https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html see multiext
- ideas
  - add # # TODO setup via fastqc metrics check
    - # for read1 in $fastqdir/*_R1.fastq.gz; do
      #     base=`basename -s _R1.fastq.gz ${read1}`
      #     filesize=$(wc -c <"$bamdir/${base}.bam")
      #     if [ $minimum_bam_size -ge $filesize ]; then
      #         echo $base >> /drive3/users/jszymanski/repos/mpnst/data/small_bams        
      #     fi
      # done
      # readarray -t small_bam < /drive3/users/jszymanski/repos/mpnst/data/small_bams         
**** Ideas
  - filter to min file size && expected by manual spreadsheet
  - fastqs too small (< 500 Mb)
    #+begin_src bash :results replace
  find /mnt/ris/aadel/mpnst/inputs/cappseq-fastq -size -500M
  #+end_src

* Local Variables
#+TODO: WAITING(w@) TODO(t) INPROCESS(p) | CLOSEOUT DONE(d!) DELEGATED(@) CANCELED(@)  
#+PROPERTY: LOGGING nil
#+PROPERTY: header-args:bash :tangle-mode (identity #o777)
#+property: header-args    :cache yes
#+property: header-args    :exports none            
#+property: header-args    :eval never-export
#+property: header-args    :results silent            
#+property: header-args    :tangle no
#+startup: shrink




