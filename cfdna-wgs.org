* Cell-free DNA Whole-Genome Sequencing                             :biopipe:
:PROPERTIES:
:header-args: :tangle no :tangle-mode (identity #o555)
:header-args+: :noweb yes
:logging: nil
:END:
** Setup
*** Bash preamble
#+name: bash_preamble
#+begin_src bash

#  Note: This script is tangled from code blocks in the Emacs Org-mode file at
#  https://github.com/jeszyman/cfdna-wgs/blob/master/cfdna-wgs.org. Changes
#  made directly to this file will be overwritten upon tangle from Org-mode.

#+end_src
*** [[file:config/int_test.yaml][Snakemake configuration YAML]]
:PROPERTIES:
:header-args:bash: :tangle ./config/int_test.yaml
:END:
#+begin_src bash
<#bash_preamble#>

blacklist: "test/inputs/hg38-blacklist.v2.bed"

container:
  default: "/home/jeszyman//sing_containers/biotools.1.0.2.sif"
  cfdna_wgs: "/home/jeszyman/sing_containers/cfdna_wgs.1.0.0.sif"

datadir: "test"

genome_fasta: "test/inputs/chr8.fa"

keepbed: "test/inputs/keep.bed"

logdir: "test/logs"

picard_jar: "/opt/picard/picard.jar"

qcdir: "test/qc"

repo:
  cfdna_wgs: "/home/jeszyman/repos/cfdna-wgs"

scriptdir:
  cfdna_wgs: "/home/jeszyman/repos/cfdna-wgs/workflow/scripts"

threads:
  default: 4
  bwa: 4

#+end_src
*** Integration testing inputs setup
#+begin_src bash
wget --directory-prefix="/home/jeszyman/repos/cfdna-wgs/test/inputs" https://raw.githubusercontent.com/Boyle-Lab/Blacklist/master/lists/hg38-blacklist.v2.bed.gz

gunzip -c ~/repos/cfdna-wgs/test/inputs/hg38-blacklist.v2.bed.gz > ~/repos/cfdna-wgs/test/inputs/hg38-blacklist.v2.bed

ls -d1 test/* | grep -v -e inputs -e ref -e fastq

ls -d ./test/

results_dirs=test/*
results_dirs=
if [ -d test/bam]
basecamp/src/smk_forced_run.sh config/int_repo_test.yaml workflow/int_test.smk
#+end_src
#+begin_src bash
#!/bin/echo Run:.

# For documentation, not intended to be executable

if [ -d test ]; then \rm -rf test; fi
mkdir -p test/fastq
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/19_2_082_R1.fastq.gz | head -n 100000 > "test/fastq/mpnst1_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/19_2_082_R2.fastq.gz | head -n 100000 > "test/fastq/mpnst1_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/25_2_072_R1.fastq.gz | head -n 100000 > "test/fastq/mpnst2_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/25_2_072_R2.fastq.gz | head -n 100000 > "test/fastq/mpnst2_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/37_JS0050CD112717_R1.fastq.gz | head -n 100000 > "test/fastq/plex1_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/37_JS0050CD112717_R2.fastq.gz | head -n 100000 > "test/fastq/plex1_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/30_JS0044CD112818_R1.fastq.gz | head -n 100000 > "test/fastq/plex2_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/30_JS0044CD112818_R2.fastq.gz | head -n 100000 > "test/fastq/plex2_R2.fastq"
for file in "test/fastq/*.fastq"; do gzip $file; done

mkdir -p "test/inputs"
wget --directory-prefix="test/inputs/" https://raw.githubusercontent.com/usadellab/Trimmomatic/main/adapters/TruSeq3-PE.fa
wget --directory-prefix="test/inputs/" https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz

cp resources/samples.tsv test/inputs/

mkdir -p test/ref
zcat "test/inputs/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz" | grep -A 2000 chr8 > test/inputs/chr8.fa
\rm test/inputs/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz

singularity shell ~/sing_containers/biotools.sif
bwa index -p test/ref/chr8 test/inputs/chr8.fa
exit


bedtools subtract -a "test/inputs/chr8.bed" -b "test/inputs/hg38-blacklist.v2.bed" > "test/inputs/keep.bed"
#+end_src
*** DONE [[file:~/repos/biotools/biotools.org::*Per-project setup work tree][Per-project setup work tree]]
** Cell-free DNA Whole-Genome Sequencing :smk:
:PROPERTIES:
:header-args:snakemake: :tangle ./workflow/cfdna_wgs.smk
:END:
*** Preamble
#+begin_src snakemake

##############################
###   cfDNA WGS Pipeline   ###
##############################

#+end_src

*** Read and alignment processing
**** Read pre-processing                                           :smk_rule:
- Snakemake
  #+begin_src snakemake
# Raw read processing with fastp
rule fastp:
    container:
        config["container"]["default"],
    input:
        read1 = cfdna_wgs_fastqs + "/{library}_raw_R1.fastq.gz",
        read2 = cfdna_wgs_fastqs + "/{library}_raw_R2.fastq.gz",
    log:
        cmd = config["logdir"] + "/{library}_fastp.log",
        html = config["logdir"] + "/{library}_fastp.html",
        json = config["logdir"] + "/{library}_fastp.json",
    output:
        read1 = cfdna_wgs_fastqs + "/{library}_processed_R1.fastq.gz",
        read2 = cfdna_wgs_fastqs + "/{library}_processed_R2.fastq.gz",
        failed = cfdna_wgs_fastqs + "/{library}_failed_fastp.fastq.gz",
        unpaired1 = cfdna_wgs_fastqs + "/{library}_unpaired_R1.fastq.gz",
        unpaired2 = cfdna_wgs_fastqs + "/{library}_unpaired_R2.fastq.gz",
    params:
        script = config["scriptdir"]["cfdna_wgs"] + "/fastp.sh",
        threads = config["threads"]["default"],
    shell:
        """
        {params.script} \
        {input.read1} \
        {input.read2} \
        {log.html} \
        {log.json} \
        {output.read1} \
        {output.read2} \
        {output.failed} \
        {output.unpaired1} \
        {output.unpaired2} \
        {params.threads} &> {log.cmd}
        """
#+end_src
- [[file:./workflow/scripts/fastp.sh][Shell script]]
  #+begin_src bash :tangle ./workflow/scripts/fastp.sh
#!/usr/bin/env bash
set -o errexit   # abort on nonzero exitstatus
set -o nounset   # abort on unbound variable
set -o pipefail  # don't hide errors within pipes

# Script variables

input_read1="${1}"
input_read2="${2}"
log_html="${3}"
log_json="${4}"
output_read1="${5}"
output_read2="${6}"
output_failed="${7}"
output_unpaired1="${8}"
output_unpaired2="${9}"
params_threads="${10}"

# Functions
main(){
    fastp_wrap $output_failed \
               $input_read1 \
               $input_read2 \
               $log_html \
               $log_json \
               $output_read1 \
               $output_read2 \
               $output_unpaired1 \
               $output_unpaired2 \
               $params_threads
}

fastp_wrap(){
    fastp --detect_adapter_for_pe \
          --failed_out $output_failed \
          --in1 $input_read1 \
          --in2 $input_read2 \
          --html $log_html \
          --json $log_json \
          --out1 $output_read1 \
          --out2 $output_read2 \
          --unpaired1 $output_unpaired1 \
          --unpaired2 $output_unpaired2 \
          --thread $params_threads
    }

# Run
main "$@"
#+end_src
**** Make alignment index                                          :smk_rule:
- Snakemake
  #+begin_src snakemake
rule index:
    container:
        config["container"]["cfdna_wgs"],
    input:
        config["genome_fasta"],
    output:
        done = touch(genome_ref)
    params:
        out_prefix = genome_ref
    shell:
        """
        bwa index -p {params.out_prefix} {input}
        """
#+end_src
**** Alignment                                                     :smk_rule:
- Snakemake
  #+begin_src snakemake
# BWA alignment
rule align:
    benchmark:
        config["logdir"] + "/{library}_align.benchmark.txt",
    container:
        config["container"]["default"],
    input:
        ref = genome_ref,
        r1 = cfdna_wgs_fastqs + "/{library}_processed_R1.fastq.gz",
        r2 = cfdna_wgs_fastqs + "/{library}_processed_R2.fastq.gz",
    log:
        config["logdir"] + "/{library}_align.log",
    output:
        sort = cfdna_wgs_bams + "/{library}_raw.bam",
        index = cfdna_wgs_bams + "/{library}_raw.bam.bai",
    params:
        script = config["scriptdir"]["cfdna_wgs"] + "/align.sh",
        threads = config["threads"]["bwa"]
    shell:
        """
        {params.script} \
        {input.ref} \
        {input.r1} \
        {input.r2} \
        {params.threads} \
        {output.sort} &> {log}
	"""
#+end_src
- Shell script
  #+begin_src bash :tangle ./workflow/scripts/align.sh
#!/usr/bin/env bash

input_ref=$1
input_r1=$2
input_r2=$3
threads=$4
output_sort=$5

bwa mem -M -t $threads \
    $input_ref \
    $input_r1 \
    $input_r2 |
    samtools view -@ $threads -Sb - -o - |
    samtools sort -@ $threads - -o $output_sort
samtools index -@ threads $output_sort
#+end_src
**** Remove PCR duplicates                                         :smk_rule:
- Snakemake
  #+begin_src snakemake
# Remove PCR duplicates from aligned reads
rule dedup:
    container:
        config["container"]["cfdna_wgs"],
    input:
        cfdna_wgs_bams + "/{library}_raw.bam",
    log:
        config["logdir"] + "/{library}_cfdna_wgs_bam_dedup.log",
    output:
        cfdna_wgs_bams + "/{library}_dedup.bam",
    params:
        script = config["scriptdir"]["cfdna_wgs"] + "/dedup.sh",
        threads = config["threads"]["bwa"],
    shell:
        """
        {params.script} \
        {input} \
        {output} \
        {threads} &> {log}
        """
#+end_src
- [[file:./workflow/scripts/dedup.sh][Shell script]]
  #+begin_src bash :tangle ./workflow/scripts/dedup.sh
#!/usr/bin/env bash
set -o errexit   # abort on nonzero exitstatus
set -o nounset   # abort on unbound variable
set -o pipefail  # don't hide errors within pipes

# Script variables
raw_bam="${1}"
dedup_bam="${2}"
threads="${3}"


samtools sort -@ $threads -n -o - $raw_bam |
    samtools fixmate -m - - |
    samtools sort -@ $threads -o - - |
    samtools markdup -@ $threads -r - $dedup_bam
samtools index $dedup_bam

#+end_src
**** Filter de-duplicated alignments                               :smk_rule:
- Snakemake
  #+begin_src snakemake
# Removes unmapped, not primary, and duplicate reads. Additionally, quality filters by config variable.
rule alignment_filtering:
    container:
        config["container"]["cfdna_wgs"],
    input:
        cfdna_wgs_bams + "/{library}_dedup.bam",
    log:
        config["logdir"] + "/{library}_alignment_filtering.log",
    output:
        cfdna_wgs_bams + "/{library}_filt.bam",
    params:
        keepbed = config["keepbed"],
        script = config["scriptdir"]["cfdna_wgs"] + "/alignment_filtering.sh",
        threads = config["threads"]["default"],
    shell:
        """
        {params.script} \
        {input} \
        {params.keepbed} \
        {params.threads} \
        {output} &> {log}
        """
#+end_src
- [[file:./workflow/scripts/alignment_filtering.sh][Shell script]]
  #+begin_src bash :tangle ./workflow/scripts/alignment_filtering.sh
#!/usr/bin/env bash

input=$1
keepbed=$2
threads=$3
output=$4

# Filter to reads that are
#  - Only mapped in proper pairs (-f 3)
#  - Excluding any unmapped, not primary alignment, or duplicates
#  - Only mapped to regions in the keep.bed file (-L $bed) (autosomes not in blacklist)
#  - Only MAPQ > 20

samtools view -@ $threads -b -f 3 -F 1284 -h -L $keepbed -M -q 20 -o $output $input

samtools index ${output}

#+end_src
*** Read and alignment QC
**** FastQC                                                        :smk_rule:
- Snakemake
  #+begin_src snakemake
# FastQC
rule fastqc:
    container:
        config["container"]["default"]
    input:
        cfdna_wgs_fastqs + "/{library}_{processing}_{read}.fastq.gz",
    log:
        config["logdir"] + "/{library}_{processing}_{read}_fastqc.log",
    output:
        config["qcdir"] + "/{library}_{processing}_{read}_fastqc.html",
        config["qcdir"] + "/{library}_{processing}_{read}_fastqc.zip",
    params:
        outdir = config["qcdir"],
        script = config["scriptdir"]["cfdna_wgs"] + "/fastqc_wrapper.sh",
        threads = config["threads"]["default"],
    shell:
        """
        {params.script} \
        {input} \
        {params.outdir} \
        {params.threads} &> {log}
        """
#+end_src
- [[file:workflow/scripts/fastqc_wrapper.sh][Shell script]]
  #+begin_src bash :tangle ./workflow/scripts/fastqc_wrapper.sh
#!/usr/bin/env bash
set -o errexit   # abort on nonzero exitstatus
set -o nounset   # abort on unbound variable
set -o pipefail  # don't hide errors within pipes

# Script variables
input="${1}"
outdir="${2}"
threads="${3}"

# Functions
fastqc  --outdir $outdir \
        --quiet \
        --threads $threads $input

#+end_src
**** Alignment QC                                                  :smk_rule:
#+begin_src snakemake
# Alignment samtools QC
rule alignment_qc:
    container:
        config["container"]["cfdna_wgs"],
    input:
        cfdna_wgs_bams + "/{library}_{processing}.bam",
    log:
        flagstat = config["logdir"] + "/{library}_{processing}_flagstat.log",
        samstat = config["logdir"] + "/{library}_{processing}_samstat.log",
    output:
        flagstat = config["qcdir"] + "/{library}_{processing}_flagstat.txt",
        samstat = config["qcdir"] + "/{library}_{processing}_samstats.txt",
    params:
        script = config["scriptdir"]["cfdna_wgs"] + "/alignment_qc.sh",
        threads = config["threads"]["default"],
    shell:
        """
        {params.script} \
        {input} \
        {log.flagstat} \
        {log.samstat} \
        {output.flagstat} \
        {output.samstat} \
        {params.threads}
        """
#+end_src
- shell
  #+begin_src bash :tangle ./workflow/scripts/alignment_qc.sh
#!/usr/bin/env bash
set -o errexit   # abort on nonzero exitstatus
set -o nounset   # abort on unbound variable
set -o pipefail  # don't hide errors within pipes

# Script variables
input="${1}"
log_flagstat="${2}"
log_samstat="${3}"
output_flagstat="${4}"
output_samstat="${5}"
threads="${6}"

# Functions
main(){
    flagstat $input $output_flagstat $log_flagstat $threads
    samstats $input $output_samstat $log_samstat $threads
}

flagstat(){
    local input="${1}"
    local output="${2}"
    local log="${3}"
    local threads="${4}"
    #
    samtools flagstat -@ $threads $input > $output 2>$log
}

samstats(){
    local input="${1}"
    local output="${2}"
    local log="${3}"
    local threads="${4}"
    #
    samtools stats -@ $threads $input > $output 2>$log
}

# Run
main "$@"
#+end_src
**** Sequencing depth metrics                                      :smk_rule:
- Snakemake
  #+begin_src snakemake
# Sequencing depth via Picard
rule picard_wgs:
    container:
        config["container"]["cfdna_wgs"],
    input:
        cfdna_wgs_bams + "/{library}_filt.bam",
    log:
        config["logdir"] + "/{library}_picard_wgs.log",
    output:
        config["qcdir"] + "/{library}_picard_wgs.txt",
    params:
        script = config["scriptdir"]["cfdna_wgs"] + "/CollectWgsMetrics_wrapper.sh",
    shell:
        """
        {params.script} \
        {input} \
        {config[picard_jar]} \
        {config[genome_fasta]} \
        {output}
        """
#+end_src
- Script
  #+begin_src bash :tangle ./workflow/scripts/CollectWgsMetrics_wrapper.sh

input=$1
picard_jar=$2
genome_fasta=$3
output=$4

java -jar $picard_jar CollectWgsMetrics \
       INPUT=$input \
       OUTPUT=$output \
       READ_LENGTH=150 \
       REFERENCE_SEQUENCE=$genome_fasta
#+end_src

**** Fragment sizes                                                :smk_rule:
- Snakemake
  #+begin_src snakemake
# Fragment sizes by deepTools
rule deeptools_bampefragmentsize:
    container:
        config["container"]["cfdna_wgs"],
    input:
        expand(cfdna_wgs_bams + "/{library}_filt.bam", library = CFDNA_WGS_LIBRARIES),
    log:
        config["logdir"] + "/bampefragmentsize.txt",
    output:
        hist = config["qcdir"] + "/deeptools_frag_lengths.png",
        raw = config["qcdir"] + "/deeptools_frag_lengths.txt",
    params:
        blacklist = config["blacklist"],
        script = config["scriptdir"]["cfdna_wgs"] + "/bamPEFragmentSize_wrapper.sh",
        threads = config["threads"]["default"],
    shell:
        """
        {params.script} \
        "{input}" \
        {log} \
        {output.hist} \
        {output.raw} \
        {params.blacklist} \
        {params.threads}
        """
#+end_src
- Script
  #+begin_src bash :tangle ./workflow/scripts/bamPEFragmentSize_wrapper.sh
#!/usr/bin/env bash
set -o errexit   # abort on nonzero exitstatus
set -o nounset   # abort on unbound variable
set -o pipefail  # don't hide errors within pipes

# Script variables

input="${1}"
log="${2}"
output_hist="${3}"
output_raw="${4}"
blacklist="${5}"
threads="${6}"


bamPEFragmentSize --bamfiles $input \
                  --numberOfProcessors $threads \
                  --blackListFileName $blacklist \
                  --histogram $output_hist \
                  --maxFragmentLength 1000 \
                  --outRawFragmentLengths $output_raw
#+end_src
**** deeptools bamCoverage                                         :smk_rule:
- Snakemake
  #+begin_src snakemake
# Make deeptools bamCoverage bedfile
rule bamcoverage:
    container:
        config["container"]["cfdna_wgs"],
    input:
        cfdna_wgs_bams + "/{library}_filt.bam",
    log:
        config["logdir"] + "/{library}_bamcoverage.log",
    output:
        config["qcdir"] + "/{library}_bamcoverage.bg",
    params:
        bin = "10000",
        blacklist = config["blacklist"],
        script = config["scriptdir"]["cfdna_wgs"] + "/bamcoverage.sh",
        threads = config["threads"]["default"],
    shell:
        """
        {params.script} \
        {input} \
        {output} \
        {params.bin} \
        {params.blacklist} \
        {params.threads} &> {log}
        """
#+end_src
- [[file:./workflow/scripts/bamcoverage.sh][Shell script]]
  #+begin_src bash :tangle ./workflow/scripts/bamcoverage.sh
#!/usr/bin/env bash

in_bam=$1
bin=$3
blacklist=$4
threads=$5
out_bg=$2

bamCoverage \
    --bam $in_bam \
    --binSize $bin \
    --blackListFileName $blacklist \
    --effectiveGenomeSize 2913022398 \
    --extendReads \
    --ignoreDuplicates \
    --ignoreForNormalization chrX \
    --normalizeUsing RPGC \
    --numberOfProcessors $threads \
    --outFileFormat bedgraph \
    --outFileName $out_bg
#+end_src
- Reference
  - https://deeptools.readthedocs.io/en/develop/content/tools/bamCoverage.html#Output
**** deeptools plotCoverage                                        :smk_rule:
- Snakemake
  #+begin_src snakemake
# deeptools plotCoverage on all filtered bams
rule plot_coverage:
    container:
        config["container"]["cfdna_wgs"],
    input:
        expand(cfdna_wgs_bams + "/{library}_filt.bam", library = CFDNA_WGS_LIBRARIES),
    log:
        config["logdir"] + "/plot_coverage.log",
    output:
        raw = config["qcdir"] + "/coverage.tsv",
        plot = config["qcdir"] + "/coverage.pdf",
    params:
        blacklist = config["blacklist"],
        script = config["scriptdir"]["cfdna_wgs"] + "/plot_coverage.sh",
    shell:
        """
        {params.script} \
        "{input}" \
        {params.blacklist} \
        {config[threads][default]} \
        {output.raw} \
        {output.plot} &> {log}
        """
#+end_src
- [[file:./workflow/scripts/plot_coverage.sh][Shell script]]
  #+begin_src bash :tangle ./workflow/scripts/plot_coverage.sh
#!/usr/bin/env bash
in_bam_string=$1
blacklist=$2
threads=$3
out_raw=$4
out_plot=$5

plotCoverage \
    --bamfiles $in_bam_string \
    --blackListFileName $blacklist \
    --extendReads \
    --numberOfProcessors $threads \
    --outRawCounts $out_raw \
    --plotFile $out_plot \
    --plotFileFormat pdf \
    --skipZeros

#+end_src

**** Multiqc                                                       :smk_rule:
- Snakemake
  #+begin_src snakemake
rule cfdna_wgs_multiqc:
    container:
        config["container"]["cfdna_wgs"],
    input:
        expand(config["logdir"] + "/{library}_fastp.json",
            library = CFDNA_WGS_LIBRARIES),
        expand(config["qcdir"] + "/{library}_{processing}_{read}_fastqc.zip",
            library = CFDNA_WGS_LIBRARIES,
            processing = ["raw", "processed", "unpaired"],
            read = ["R1","R2"]),
        expand(config["qcdir"] + "/{library}_{processing}_flagstat.txt",
            library = CFDNA_WGS_LIBRARIES,
            processing = ["raw", "dedup", "filt"]),
        expand(config["qcdir"] + "/{library}_{processing}_samstats.txt",
            library = CFDNA_WGS_LIBRARIES,
            processing = ["raw", "dedup", "filt"]),
        expand(config["qcdir"] + "/{library}_picard_wgs.txt",
            library = CFDNA_WGS_LIBRARIES),
        config["qcdir"] + "/deeptools_frag_lengths.txt",
        config["qcdir"] + "/coverage.tsv",
    log:
        config["logdir"] + "/cfdna_wgs_multiqc.log"
    output:
        config["qcdir"] + "/all_cfdna_wgs.html",
    params:
        out_dir = config["qcdir"],
        out_name = "all_cfdna_wgs",
        script = config["scriptdir"]["cfdna_wgs"] + "/cfdna_wgs_multiqc.sh",
    shell:
        """
        {params.script} \
        "{input}" \
        {params.out_name} \
        {params.out_dir} &> {log}
        """

#+end_src
- Shell
  #+begin_src bash :tangle ./workflow/scripts/cfdna_wgs_multiqc.sh
#!/usr/bin/env bash
set -o errexit   # abort on nonzero exitstatus
set -o nounset   # abort on unbound variable
set -o pipefail  # don't hide errors within pipes

# Script variables

   input="${1}"
out_name="${2}"
 out_dir="${3}"

# Functions

multiqc $input \
        --force \
        --outdir $out_dir \
        --filename $out_name

#+end_src
**** Make QC table                                                 :smk_rule:
- Snakemake
  #+begin_src snakemake
#  Notes:
#  This makes an aggregate table of QC values. The subsequent downsampling
#  step only runs if read numbers are above a certain threshold. See also
#  the int_test.smk for function using this output table.

checkpoint make_qc_tbl:
    container:
        config["container"]["cfdna_wgs"],
    input:
        fq = config["qcdir"] + "/all_cfdna_wgs_data/multiqc_fastqc.txt",
        sam = config["qcdir"] + "/all_cfdna_wgs_data/multiqc_samtools_stats.txt",
        flag = config["qcdir"] + "/all_cfdna_wgs_data/multiqc_samtools_flagstat.txt",
	picard = config["qcdir"] + "/all_cfdna_wgs_data/multiqc_picard_wgsmetrics.txt",
        deeptools_frag = config["qcdir"] + "/deeptools_frag_lengths.txt",
        deeptools_cov = config["qcdir"] + "/coverage.tsv"
    log:
        config["logdir"] + "/read_qc.log"
    output:
        readqc = config["qcdir"] + "/cfdna_wgs_read_qc.tsv",
        fraglen = config["qcdir"] + "/cfdna_wgs_frag_len.tsv",
    params:
        script = config["scriptdir"]["cfdna_wgs"] + "/make_qc_tbl.R"
    shell:
        """
        Rscript {params.script} \
        {input.fq} \
        {input.sam} \
        {input.flag} \
        {input.picard} \
        {input.deeptools_frag} \
        {input.deeptools_cov} \
        {output.readqc} \
        {output.fraglen} >& {log}
        """
#+end_src
- Rscript
  #+begin_src R :tangle ./workflow/scripts/make_qc_tbl.R
#fastqc_input="test/qc/all_cfdna_wgs_data/multiqc_fastqc.txt"
## samstats_input="test/qc/all_cfdna_wgs_data/multiqc_samtools_stats.txt"
## flagstats_input="test/qc/all_cfdna_wgs_data/multiqc_samtools_flagstat.txt"
## picard_input="test/qc/all_cfdna_wgs_data/multiqc_picard_wgsmetrics.txt"
deeptools_frag_input="test/qc/deeptools_frag_lengths.txt"

args = commandArgs(trailingOnly = TRUE)
fastqc_input = args[1]
samstats_input = args[2]
flagstats_input = args[3]
picard_input = args[4]
deeptools_frag_input = args[5]
deeptools_cov_input = args[6]
readqc_out_tbl = args[7]
frag_len_out_tbl = args[8]

library(tidyverse)

process_multiqc_fastqc = function(multiqc_fastqc_input){
  as_tibble(read.table(multiqc_fastqc_input, header = TRUE, sep = '\t', stringsAsFactors = FALSE)) %>%
  mutate(library = substr(Filename,1,6)) %>%
  mutate(read = ifelse(grepl("R1", Filename), "read1", "read2")) %>%
  mutate(fastq_processing = gsub("_.*$","",substr(Sample, 8, length(Sample)))) %>%
  select(!c(Sample,File.type,Encoding)) %>%
  pivot_wider(
    names_from = c(read,fastq_processing),
    values_from = !c(library,read,fastq_processing))
}

fastqc = process_multiqc_fastqc(fastqc_input)

process_multiqc_samfile = function(multiqc_samfile){
  as_tibble(read.table(multiqc_samfile, header = TRUE, sep = '\t', stringsAsFactors = FALSE)) %>%
  mutate(library = substr(Sample, 1, 6)) %>%
  mutate(bam_processing = gsub("_.*$","",substr(Sample, 8, length(Sample)))) %>%
  select(!c(Sample)) %>%
  pivot_wider(
    names_from = c(bam_processing),
    values_from = !c(library, bam_processing))
}

samstats = process_multiqc_samfile(samstats_input)
flagstats = process_multiqc_samfile(flagstats_input)

deeptools_frag = read_tsv(deeptools_frag_input, col_names = c("frag_len","frag_count","file"), skip = 1) %>%
  filter(frag_len < 500) %>%
  mutate(library = substr(gsub("^.*lib", "lib", file), 1,6)) %>%
  mutate(frag_len = sub("^", "frag_len", frag_len)) %>%
  select(library, frag_len, frag_count) %>%
  pivot_wider(
    names_from = frag_len,
    values_from = frag_count)

picard = as_tibble(read.table(picard_input, header = TRUE, sep = '\t', stringsAsFactors = FALSE)) %>%
  mutate(library = Sample)

deeptools_cov = read_tsv(deeptools_cov_input, skip = 1) %>%
  pivot_longer(!c(`#'chr'`, `'start'`,`'end'`), names_to = "file", values_to = "cnt") %>%
  rename(chr = `#'chr'`,
         start = `'start'`,
         end = `'end'`) %>%
  mutate(library = substr(file, 2, 7)) %>%
  group_by(library) %>%
  summarise(
    mean_cov = mean(cnt),
    median_cov = median(cnt),
            )

readqc = fastqc %>%
  left_join(samstats, by = "library") %>%
  left_join(flagstats, by = "library") %>%
  left_join(deeptools_frag, by = "library") %>%
  left_join(picard, by = "library") %>%
  left_join(deeptools_cov, by = "library")

write.table(readqc, file = readqc_out_tbl, row.names = F, sep = '\t', quote = F)

all_frag_len = data.frame(frag_len = 1:500)

frag_len =
  readqc %>% select(starts_with("frag_len") | matches("library")) %>%
  pivot_longer(!library, names_to = "frag_len", values_to = "count") %>%
  mutate(frag_len = as.numeric(gsub("frag_len","",frag_len))) %>%
  mutate(count = as.numeric(count)) %>%
  pivot_wider(names_from = library, values_from = count) %>%
  right_join(all_frag_len) %>% arrange(frag_len) %>%
  replace(is.na(.), 0)

write_tsv(frag_len, file = frag_len_out_tbl)
#+end_src

** Integration testing [[file:workflow/int_test.smk][Snakefile]] :smk:
:PROPERTIES:
:header-args:snakemake: :tangle ./workflow/int_test.smk :tangle-mode (identity #o555)
:END:
*** Preamble, variable naming and functions
#+begin_src snakemake

##################################################################
###   Integration testing snakefile for WGS cfDNA Processing   ###
##################################################################

import pandas as pd
import re
import numpy as np
#container: config["container"]

# Setup sample name index as a python dictionary

libraries = pd.read_table(config["datadir"] + "/inputs/libraries.tsv")

readable = []
for x in libraries.file:
    readable.append(os.access(x, os.R_OK))
libraries['readable']=readable

cfdna_libraries = libraries
cfdna_libraries = cfdna_libraries[cfdna_libraries.library_type == "wgs"]
cfdna_libraries = cfdna_libraries[cfdna_libraries.isolation_type == "cfdna"]
cfdna_libraries = cfdna_libraries[cfdna_libraries.readable == True]

library_indict = cfdna_libraries["library"].tolist()
file_indict = cfdna_libraries["file"].tolist()
lib_dict = dict(zip(library_indict, file_indict))

CFDNA_WGS_LIBRARIES = list(lib_dict.keys())
CFDNA_WGS_FASTQS = list(lib_dict.values())

# Makes the name bwa index directory from the config genome fasta
#  e.g. test/inputs/chr8.fa will make test/ref/chr8
genome_ref = config["genome_fasta"]
genome_ref = re.sub("inputs", lambda x: 'ref', genome_ref)
genome_ref = re.sub("\..*$", lambda x: '', genome_ref)

# Directory structure under datadir:
cfdna_wgs_fastqs = config["datadir"] + "/cfdna-wgs-fastqs"
cfdna_wgs_bams =  config["datadir"] + "/cfdna-wgs-bams"
cfdna_wgs_failed_fastp = config["datadir"] + "/fastq_failed"

#+end_src
*** All rule
#+begin_src snakemake

#########1#########2#########3#########4#########5#########6#########7#########8

rule all:
    input:
        expand(cfdna_wgs_fastqs +
           "/{library}_{processing}_{read}.fastq.gz",
            library = lib_dict.keys(),
            processing = ["raw", "processed", "unpaired"],
            read = ["R1", "R2"]),
        expand(cfdna_wgs_fastqs + "/{library}_failed_fastp.fastq.gz",
            library = CFDNA_WGS_LIBRARIES),
        expand(config["qcdir"] + "/{library}_{processing}_{read}_fastqc.html",
            library = CFDNA_WGS_LIBRARIES,
            processing = ["raw", "processed", "unpaired"],
            read = ["R1", "R2"]),
        genome_ref,
        expand(cfdna_wgs_bams + "/{library}_{processing}.bam",
            library = CFDNA_WGS_LIBRARIES,
            processing = ["raw", "dedup", "filt"]),
        expand(config["qcdir"] + "/{library}_{processing}_flagstat.txt",
            library = CFDNA_WGS_LIBRARIES,
            processing = ["raw", "dedup", "filt"]),
        expand(config["qcdir"] + "/{library}_{processing}_samstats.txt",
            library = CFDNA_WGS_LIBRARIES,
            processing = ["raw", "dedup", "filt"]),
        expand(config["qcdir"] + "/{library}_picard_wgs.txt", library = CFDNA_WGS_LIBRARIES),
        config["qcdir"] + "/deeptools_frag_lengths.png",
        config["qcdir"] + "/deeptools_frag_lengths.txt",
        expand(config["qcdir"] + "/{library}_bamcoverage.bg", library = CFDNA_WGS_LIBRARIES),
        config["qcdir"] + "/coverage.tsv",
        config["qcdir"] + "/coverage.pdf",
        config["qcdir"] + "/all_cfdna_wgs.html",
        config["qcdir"] + "/cfdna_wgs_read_qc.tsv",
        config["qcdir"] + "/cfdna_wgs_read_qc.tsv",
        config["qcdir"] + "/cfdna_wgs_frag_len.tsv",
#+end_src
*** Symlink input fastqs
#+begin_src snakemake
rule symlink_inputs:
    container:
        config["container"]["default"],
    input:
        lambda wildcards: lib_dict[wildcards.library],
    output:
        read1 = cfdna_wgs_fastqs + "/{library}_raw_R1.fastq.gz",
        read2 = cfdna_wgs_fastqs + "/{library}_raw_R2.fastq.gz",
    params:
        outdir = cfdna_wgs_fastqs,
        script = config["scriptdir"]["cfdna_wgs"] + "/symlink.sh",
    shell:
        """
        {params.script} \
        {input} \
        {output.read1} \
        {output.read2} \
        {params.outdir}
        """
#+end_src
#+begin_src bash :tangle ./workflow/scripts/symlink.sh
#!/usr/bin/env bash
set -o errexit   # abort on nonzero exitstatus
set -o nounset   # abort on unbound variable
set -o pipefail  # don't hide errors within pipes

# Script variables
input_read1="${1}"
output_read1="${2}"
output_read2="${3}"
outdir="${4}"

mkdir -p $outdir

input_read2=$(echo $input_read1 | sed "s/_R1/_R2/g")

ln -sf --relative $input_read1 $output_read1
ln -sf --relative $input_read2 $output_read2
#+end_src
*** Includes statements
#+begin_src snakemake
include: config["repo"]["cfdna_wgs"] + "/workflow/cfdna_wgs.smk"
#+end_src

** [[file:resources/int_test.pdf]]

** [[file:README.md][README]]
:PROPERTIES:
:export_file_name: ./README.md
:export_options: toc:nil ^:nil
:END:
*** Introduction
This repository has a snakemake workflow for basic processing of whole-genome sequencing reads from cell-free DNA.

[[file:resources/int_test.png]]

Master branch of the repository contains most recent developments. Stable versions are saved as terminal branches (/e.g./ stable1.0.0).

Files labeled int_test will run integration testing of all rules on a small dataset in test/inputs. See config/int_test.yaml for necessary run conditions.

*** Changlog
- [2022-09-08 Thu] - Version 5.3.0: some minor name changes
- [2022-08-19 Fri] - Version 5.2.0 validated: Adds bamCoverage and plotCoverage from deeptools. Benchmarks BWA.
- [2022-08-09 Tue] - Version 5.1.0 validated: Added cfdna wgs-specific container for each rule, referenced to config
- [2022-08-05 Fri] - Version 5.0.0 validated: Added a symlink rule based on python dictionary. Added repo-specific output naming, added checks for sequence type and file readability to input tsv.
- [2022-06-27 Mon] - Version 4 validated. Further expanded read_qc.tsv table. Removed bam post-processing step and added a more expansive bam filtering step. Updated downsampling to work off filtered alignments.
- [2022-06-26 Sun] - Version 3.2 validated. Expanded the qc aggregate table and added some comments.
- [2022-06-24 Fri] - Validate version 3.1 which includes genome index build as a snakefile rule.
- [2022-06-24 Fri] - Validated version 3 with read number checkpoint for down-sampling.
- [2022-05-31 Tue] - Conforms to current biotools best practices.
- [2022-04-29 Fri] - Moved multiqc to integration testing as inputs are dependent on final sample labels. Integration testing works per this commit.
** :dev:
:PROPERTIES:
:header-args: :tangle no
:END:
*** TODO 6.0.0 pull in cna
*** Ideas
- Prioritized [2022-06-07 Tue]

- update aggregate qc table
- expand seq depth metrics
  - using mosdepth
    #+name: mosdepth
    #+begin_src bash
  #########1#########2#########3#########4#########5#########6#########7#########8
  #
  ### mosdepth for WGS depth calc  ###
  #
  # Setup
  ##

  # Mosdepth per bam dir
  ##
  ## For deduped bams
  for file in $localdata/bams/*.dedup.sorted.bam; do
      mosdepth_mpnst $file $localdata/bam-qc/dedup 250000000
  done
  ##
  #
  # get simple tsv and send to repo

  for file in $localdata/bam-qc/dedup/lib*.regions.bed.gz; do
      base=`basename -s .dedup.sorted.regions.bed.gz $file`
      zcat $file | awk -v FS='\t' -v var=$base 'NR <=24 {print var,$1,$4}' >> $localdata/bam-qc/dedup/all_dedup_coverage
  done

  header=library_id\\tchr\\tmean_coverage
  sed -i "1 i$header" $localdata/bam-qc/dedup/all_dedup_coverage

  ## Local
  >>>>>>> 2d6bf2d62424a76f5893600fce7444a867784228
  source ~/repos/mpnst/bin/local-setup.sh
  docker_interactive
  biotools
  ##
  ## Functions
  ###
  ### Convert bams to wigs
  bam_to_wig() {
      printf "Variables are: 1=bam_file 2=bam_suffix 3=outdir\n"
          base=`basename -s ${2} $1`
          if [ $3/${base}.wig -ot $1 ]; then
              /opt/hmmcopy_utils/bin/readCounter --window 1000000 --quality 20 \
                                                 --chromosome "chr1,chr2,chr3,chr4,chr5,chr6,chr7,chr8,chr9,chr10,chr11,chr12,chr13,chr14,chr15,chr16,chr17,chr18,chr19,chr20,chr21,chr22,chrX,chrY" $1 > $3/${base}.wig
          fi
  }
  ###
  ### Run ichor for low TF
  ichor_lowfract() {
      base=`basename -s .wig $1`
      if [ $2/$base.RData -ot $1 ]; then
          Rscript /opt/ichorCNA/scripts/runIchorCNA.R \
                  --id $base \
                  --WIG $1 \
                  --gcWig /opt/ichorCNA/inst/extdata/gc_hg19_1000kb.wig \
                  --normal "c(0.95, 0.99, 0.995, 0.999)" \
                  --ploidy "c(2)" \
                  --maxCN 3 \
                  --estimateScPrevalence FALSE \
                  --scStates "c()" \
                  --outDir $2
      fi
  }
  ##
  ##
  mkdir -p $localdata/wigs
  mkdir -p $localdata/ichor
  #
  # Make wigs
  #
  #bam_to_wig /mnt/xt3/mpnst/frag-filt-bams/lib109.dedup.sorted.frag90_150.sorted.bam .dedup.sorted.frag90_150.sorted.bam $localdata/wigs
  ##
  for file in $localdata/frag-filt-bams/lib109*.bam; do
      bam_to_wig $file \
                 .dedup.sorted.frag.sorted.bam \
                 $localdata/wigs
  done

  ## For fraction-filtered WGS cfDNA
  for file in $localdata/frag-filt-bams/*.bam; do
      bam_to_wig $file \
                 .dedup.sorted.frag.sorted.bam \
                 $localdata/wigs
  done
  ##
  ## For tumor and leukocyte WGS libraries
  ### Make array of genomic library file paths
  genomic=($(cat /drive3/users/jszymanski/repos/mpnst/data/libraries.csv | grep -e tumor -e leukocyte | grep -v "wes" | awk -F, '{print $1}' | sed 's/"//g' | sed 's/$/.dedup.sorted.bam/g' | sed 's/^/\/mnt\/xt3\/mpnst\/bams\//g'))
  ###
  for file in ${genomic[@]}; do
      bam_to_wig $file \
                 .dedup.sorted.bam \
                 $localdata/wigs
  done
  #
  ##
  ## Send successful file list to repo
  rm /drive3/users/jszymanski/repos/mpnst/data/wigs.tsv
  for file in $localdata/wigs/*.wig;
  do
      base=`basename -s .wig $file`
      echo $base >> /drive3/users/jszymanski/repos/mpnst/data/wigs.tsv
  done
  #
  ##RESUME HERE
  # ichor
  ##
  for file in $localdata/wigs/lib109*.wig; do
      ichor_lowfract $file $localdata/ichor
  done


  header=library_id\\tchr\\tmean_coverage
  sed -i "1 i$header" $localdata/bam-qc/dedup/all_dedup_coverage

  max_file_size=5000000
  file_size=$(
      wc -c <"$localdata/bam-qc/dedup/all_dedup_coverage"
           )

  if [ $filesize -gt $max_file_size ]; then
      touch $repo/data/qc/all_dedup_coverage_too_big
  else
      cp $localdata/bam-qc/dedup/all_dedup_coverage $repo/qc/all_dedup_coverage.tsv
  fi
  #
  #+end_src
    - Cant calcualte depths off [[file:~/repos/mpnst/data/bam_qc_data/mqc_mosdepth-coverage-per-contig_1.txt]] , d/n allow values under 1
    - [ ] for coverage, should intersect down to autosomes
    - https://github.com/brentp/mosdepth
    - run and extract mosdepth
      mosdepthRAW = as_tibble(read.table(file.path(repo,"data/all_dedup_coverage.tsv"), header = T, sep = '\t', fill = TRUE))

*** Dev                                                                 :dev:
:PROPERTIES:
:header-args:snakemake: :tangle no
:END:
****  <RULE DESCRIPTIVE ORG NAME>                                  :smk_rule:
- Snakemake
  #+begin_src snakemake
# Post-alignment processing
# Post-processing with samblaster and samtools
# Final bam is duplicate marked (NOT removed), location sorted

rule <RULE WORKFLOW NAME>:
    container:
        < SMK ID >_container,
    input:
        < SMK ID >_< PROCESS ID > + "/{<UNIT ID>}<INPUT SUFFIX",
    log:
        config["logdir"] + "/{<UNIT ID>}_< SMK ID >_<RULE WORKFLOW NAME>.log",
    output:
        < SMK ID >_<OUTPUT DIRECTORY IDENTIFIER> + "/{<UNIT ID>}<OUTPUT SUFFIX>",
    params:
        script = config["scriptdir"]["< SMK ID >"],
    shell:
        """
        {params.script} \
        {input} \
        {output} &> {log}
        """
#+end_src
- [[file:./workflow/scripts/<rule workflow name>.sh][Shell script]]
  #+begin_src bash :tangle ./workflow/scripts/<rule workflow name>.sh
#!/usr/bin/env bash
input=$1
output=$2
#+end_src

**** Make aggregate fragment table                                 :smk_rule:
- Snakemake
  #+begin_src snakemake
rule aggregate_frag:
    container:
        config["container"]["cfdna_wgs"],
    input:
        expand(config["qcdir"] + "/{library}_deeptools_frag_lengths.txt", library = CFDNA_WGS_LIBRARIES),
    log:
        config["logdir"] + "/aggregate_frag.err",
    output:
        config["qcdir"] + "/all_cfdna_wgs_frag.tsv",
    shell:
        """
        awk 'FNR>2' {input} > {output} 2> {log}
        """
#+end_src

*** DONE Downsample bams                                           :smk_rule:
- Snakemake
  #+begin_src snakemake
# Alignment downsampling
#  Note: Used for all rule input "get_ds_candidates". See that function in
#  workflow/int_test.smk

rule downsample_bams:
    input:
        cfdna_wgs_bam_dir + "/filt/{library_id}_filt.bam",
    output:
        cfdna_wgs_bam_dir + "/ds/{library_id}_ds{milreads}.bam",
    log:
        config["logdir"] + "/downsample_bam_{library_id}_{milreads}.err"
    container:
        config["cfdna_wgs_container"]
    shell:
        """
        {config[cfdna_wgs_script_dir]}/downsample_bam.sh {input} {wildcards.milreads} {output} 2>{log}
        """
#+end_src
- Shell script
  #+begin_src bash :tangle ./workflow/scripts/downsample_bam.sh
## Calculate the sampling factor based on the intended number of reads:
FACTOR=$(samtools idxstats $1 | cut -f3 | awk -v COUNT=$2 'BEGIN {total=0} {total += $1} END {print COUNT/total}')

if [[ $FACTOR > 1 ]]; then
    echo "DS reads exceeds total for $1"
    cp $1 $3
else
    sambamba view -s $FACTOR -f bam -l 5 $1 > $3
fi
#+end_src
  #+begin_src bash
# Collect only deduped, mapped, paired reads of >q20
samtools idxstats test/bam/lib001.bam | cut -f 1 | grep -vE 'chrM|_random|chrU|chrEBV|\*' | \
xargs samtools view -f 1 -F 1284 -q 20 -o /tmp/test.bam test/bam/lib001.bam

# From this high-quality subset, perform downsampling to a set number of reads:
FACTOR=$(samtools idxstats $1 | cut -f3 | awk -v COUNT=$2 'BEGIN {total=0} {total += $1} END {print COUNT/total}')

if [[ $FACTOR > 1 ]]; then
    echo "DS reads exceeds total for $1"
else
samtools idxstats in.bam | cut -f 1 | grep -vE 'chrM|_random|chrU|chrEBV|\*' | \
xargs samtools view -f 1 -F 1284 -q 20 -o out.bam in.bam
    sambamba view -s $FACTOR -f bam -l 5 $1 > $3
fi
#+end_src
** :ref:
:PROPERTIES:
:header-args: :tangle no
:END:
- https://github.com/jeszyman/cfdna-wgs
- [[id:271b4d5f-727e-496e-b835-8fe9f8655655][Bioinformatics project module]]
*** [[id:13120759-71db-497c-8ed3-1c58e47a7840][Biotools headline]]
*** Old rules
**** DONE Alignment processing                                     :smk_rule:
#+begin_src snakemake
# Alignment deduplication and sorting
rule alignment_processing:
    input:
        config["datadir"] + "/bam/{library_id}_raw.bam",
    output:
        dedup = temp(config["datadir"] + "/bam/{library_id}_dedup_unsort.bam"),
        sort = config["datadir"] + "/bam/{library_id}_dedup.bam",
        index = config["datadir"] + "/bam/{library_id}_dedup.bam.bai",
    log:
        config["datadir"] + "/logs/alignment_processing_{library_id}.log"
    shell:
        """
        {config[cfdna_wgs_script_dir]}/alignment_processing.sh \
        {input} \
        {config[threads]} \
        {output.bam} \
        {output.dedup} \
        {output.sort} \
        {output.index} \
        &> {log}
        """
#+end_src
- [[file:workflow/scripts/alignment_processing.sh][Script]]
  #+begin_src bash :tangle ./workflow/scripts/alignment_processing.sh
#!/usr/bin/env bash

<#bash_preamble#>

input=$1
threads=$2
output_bam=$3
output_dedup=$4
output_sort=$5
output_index=$6

sambamba view -t $threads -S -f bam $input > $output_bam
sambamba markdup -r -t $threads $output_bam $output_dedup
sambamba sort -t $threads $output_dedup -o $output_sort
sambamba index -t $threads $output_sort

#+end_src
