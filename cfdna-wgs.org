* Cell-free DNA Whole-Genome Sequencing                             :biopipe:
:PROPERTIES:
:header-args:bash: :tangle-mode (identity #o555)
:logging: nil
:END:
** Setup
#+begin_src bash
ls -d1 test/* | grep -v -e inputs -e ref -e fastq

ls -d ./test/

results_dirs=test/*
results_dirs=
if [ -d test/bam]
basecamp/src/smk_forced_run.sh config/int_repo_test.yaml workflow/int_test.smk
#+end_src
*** Snakemake
**** Configuration YAMLs
- Repo integration testing
  #+begin_src bash :tangle ./config/int_repo_test.yaml
container: "~/sing_containers/biotools.sif"
fq_dir: "test/fastq"
qc_dir: "test/qc"
log_dir: "test/logs"
threads: 4
inputs_dir: "test/inputs"
processed_fq_dir: "test/processed-fastq"
unpr_fq_dir: "test/unpaired-fastq"
bwa_index: "test/ref/chr8"
bam_dir: "test/bam"
rename_dir: "test/cappseq_fastq"
extracted_dir: "test/extracted_fastq"
fq_symlink_dir: "test/symlink-fastq"
MILREADS:
  - "10"
  - "20"
cfdna_wgs_script_dir: "workflow/scripts"
#+end_src


*** Integration testing inputs setup
#+begin_src bash :tangle ./src/seq_preprocess_integration_setup.sh
#!/bin/echo Run:.

# For documentation, not intended to be executable

if [ -d test ]; then \rm -rf test; fi
mkdir -p test/fastq
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/19_2_082_R1.fastq.gz | head -n 100000 > "test/fastq/mpnst1_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/19_2_082_R2.fastq.gz | head -n 100000 > "test/fastq/mpnst1_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/25_2_072_R1.fastq.gz | head -n 100000 > "test/fastq/mpnst2_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/25_2_072_R2.fastq.gz | head -n 100000 > "test/fastq/mpnst2_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/37_JS0050CD112717_R1.fastq.gz | head -n 100000 > "test/fastq/plex1_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/37_JS0050CD112717_R2.fastq.gz | head -n 100000 > "test/fastq/plex1_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/30_JS0044CD112818_R1.fastq.gz | head -n 100000 > "test/fastq/plex2_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/30_JS0044CD112818_R2.fastq.gz | head -n 100000 > "test/fastq/plex2_R2.fastq"
for file in "test/fastq/*.fastq"; do gzip $file; done

mkdir -p "test/inputs"
wget --directory-prefix="test/inputs/" https://raw.githubusercontent.com/usadellab/Trimmomatic/main/adapters/TruSeq3-PE.fa
wget --directory-prefix="test/inputs/" https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz

cp resources/samples.tsv test/inputs/

mkdir -p test/ref
zcat "test/inputs/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz" | grep -A 2000 chr8 > test/inputs/chr8.fa
\rm test/inputs/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz

singularity shell ~/sing_containers/biotools.sif
bwa index -p test/ref/chr8 test/inputs/chr8.fa
exit
#+end_src
** README
*** Changlog
- [2022-04-29 Fri] - Moved multiqc to integration testing as inputs are dependent on final sample labels. Integration testing works per this commit.
** [[file:workflow/read_preprocess.smk][Sequence pre-processing, alignment, and quality control]] :smk:
:PROPERTIES:
:header-args:snakemake: :tangle ./workflow/read_preprocess.smk
:END:
*** DONE Read pre-processing                                          :smk_rule:
- Snakemake
  #+begin_src snakemake
rule trimmomatic:
    input:
        read1 = config["fq_dir"] + "/{read_id}_R1.fastq.gz",
        read2 = config["fq_dir"] + "/{read_id}_R2.fastq.gz",
    params:
        adapter_fasta = config["inputs_dir"] + "/TruSeq3-PE.fa",
    output:
        read1 = config["processed_fq_dir"] + "/{read_id}_proc_R1.fastq.gz",
        read1_unpr = config["unpr_fq_dir"] + "/{read_id}_unpr_R1.fastq.gz",
        read2 = config["processed_fq_dir"] + "/{read_id}_proc_R2.fastq.gz",
        read2_unpr = config["unpr_fq_dir"] + "/{read_id}_unpr_R2.fastq.gz",
    log:
        int = config["log_dir"] + "/trimmomatic_trimlog_{read_id}.log",
        main = config["log_dir"] + "/trimmomatic_{read_id}.log",
    shell:
        """
        trimmomatic PE \
                    -threads {config[threads]} \
                    -trimlog {log.int} \
                    {input.read1} {input.read2} \
                    {output.read1} {output.read1_unpr} \
                    {output.read2} {output.read2_unpr} \
                    ILLUMINACLIP:{params.adapter_fasta}:2:30:10 \
                    LEADING:10 TRAILING:10 MAXINFO:50:0.97 MINLEN:20 &> {log.main}
        """
#+end_src
- Reference
  - Trimmomatic parameters based on Taylor's parameters ([[https://mail.google.com/mail/u/0/#search/sundby+fastq/FMfcgzGmvLWSbsmhDsffvSSWfjWdQhhR?projector=1&messagePartId=0.1][email]])
  - https://github.com/AAFC-BICoE/snakemake-trimmomatic/blob/master/Snakefile
*** DONE Alignment
#+begin_src snakemake
rule align:
    input:
        read1 = config["processed_fq_dir"] + "/{read_id}_proc_R1.fastq.gz",
        read2 = config["processed_fq_dir"] + "/{read_id}_proc_R2.fastq.gz",
    output:
        config["bam_dir"] + "/{read_id}.sam",
    log:
        config["log_dir"] + "/align_{read_id}.log"
    shell:
        """
        bwa mem -M -t 4 {config[bwa_index]} {input.read1} {input.read2} > {output}
	"""
#+end_src

*** DONE FastQC                                                       :smk_rule:
- Snakemake
  #+begin_src snakemake
rule fastqc:
    input:
        raw=config["fq_dir"] + "/{read_id}_{read}.fastq.gz",
        proc=config["processed_fq_dir"] + "/{read_id}_proc_{read}.fastq.gz",
    params:
        out_dir = config["qc_dir"],
    output:
        raw_html = config["qc_dir"] + "/{read_id}_{read}_fastqc.html",
        proc_html = config["qc_dir"] + "/{read_id}_proc_{read}_fastqc.html",
    log:
        raw = config["log_dir"] + "/fastqc_raw_{read_id}_{read}.log",
        proc = config["log_dir"] + "/fastqc_proc_{read_id}_{read}.log",
    shell:
        """
        fastqc --outdir {params.out_dir} \
        --quiet \
        --threads {config[threads]} {input.raw} &> {log}
        fastqc --outdir {params.out_dir} \
        --quiet \
        --threads {config[threads]} {input.proc} &> {log}
        """
#+end_src
*** DONE Alignment processing
#+begin_src snakemake
rule alignment_processing:
    input:
        config["bam_dir"] + "/{read_id}.sam",
    output:
        bam = config["bam_dir"] + "/{read_id}_raw.bam",
        dedup = temp(config["bam_dir"] + "/{read_id}_dedup_unsort.bam"),
        sort = config["bam_dir"] + "/{read_id}_dedup.bam",
        index = config["bam_dir"] + "/{read_id}_dedup.bam.bai",
    log:
        config["log_dir"] + "/alignment_processing_{read_id}.log"
    shell:
        """
        sambamba view -t {config[threads]} -S -f bam {input} > {output.bam}
        sambamba markdup -r -t {config[threads]} {output.bam} {output.dedup}
        sambamba sort -t {config[threads]} {output.dedup} -o {output.sort}
        sambamba index -t {config[threads]} {output.sort}
        """
#+end_src
*** DONE Alignment QC
#+begin_src snakemake
rule alignment_qc:
    input:
        config["bam_dir"] + "/{read_id}_{bam_step}.bam",
    output:
        samstat = config["qc_dir"] + "/{read_id}_{bam_step}_samstats.txt",
        flagstat = config["qc_dir"] + "/{read_id}_{bam_step}_flagstat.txt",
    shell:
        """
        samtools stats {input} > {output.samstat}
        samtools flagstat {input} > {output.flagstat}
        """
#+end_src
*** DONE Downsample bams
- fails in mpnst int testing- "/bin/bash: line 1: cfdna-wgs/workflow/scripts/downsample_bam.sh: No such file or directory"
#+begin_src snakemake
rule downsample_bams:
    input:
        config["bam_dir"] + "/{read_id}_dedup.bam",
    output:
        config["bam_dir"] + "/{read_id}_ds{milreads}.bam",
    shell:
        """
        {config[cfdna_wgs_script_dir]}/downsample_bam.sh {input} {wildcards.milreads}000000 {output}
        """
#+end_src

#+begin_src bash :tangle ./workflow/scripts/downsample_bam.sh
## Calculate the sampling factor based on the intended number of reads:
FACTOR=$(samtools idxstats $1 | cut -f3 | awk -v COUNT=$2 'BEGIN {total=0} {total += $1} END {print COUNT/total}')

if [[ $FACTOR > 1 ]]; then
    echo "DS reads exceeds total for $1"
    cp $1 $3
else
    sambamba view -s $FACTOR -f bam -l 5 $1 > $3
fi
#+end_src

*** Dev
:PROPERTIES:
:header-args:snakemake: :tangle no
:END:
**** Ideas
**** Ideas

:PROPERTIES:
:header-args:snakemake: :tangle no
:END:
- https://github.com/brentp/mosdepth
- https://deeptools.readthedocs.io/en/develop/content/tools/bamCoverage.html#Output
Mem cap
Config to run command

- frag size
  #+name: fragment_size.sh
#+begin_src bash
#########1#########2#########3#########4#########5#########6#########7#########8
#
source ./src/setup.sh
docker_interactive
jeszyman
biotools
source ~/repos/mpnst/src/setup.sh

# Function
mpnst_fragsize() {
    bamPEFragmentSize --bamfiles $1 \
                      --numberOfProcessors $2 \
                      --binSize $3 \
                      --distanceBetweenBins $4 \
                      --outRawFragmentLengths $5
}

##
## Local variables
processors=40
bin_size=10000000
distance_between_bins=10000000
min_bam_size=100000000

#
# Generate bam file lists
#  Note: Small or empty bams kill bamPEFragmentSize and must be excluded
##
## For fragment-filtered bams
declare -a frag_filt_bam=()
for file in $localdata/frag-filt-bams/*.dedup.sorted.frag.sorted.bam;
do
    bamsize=$(wc -c <"$file")
    if [ $bamsize -ge $min_bam_size ]; then
        frag_filt_bam+=("$file")
    fi
done
##
## For deduped full bams
declare -a dedup_bam
for file in $localdata/bams/*.dedup.sorted.bam;
do
    bamsize=$(wc -c <"$file")
    if [ $bamsize -ge $min_bam_size ]; then
        dedup_bam+=("$file")
    fi
done
##
mkdir -p $localdata/frag_size
#
for file in "${frag_filt_bam[@]}";
do
    base=`basename $file`
    if [ $localdata/frag_size/${base}.fragsize.tsv -ot $file ]; then
        mpnst_fragsize \
            $file \
            $processors \
            $bin_size \
            $distance_between_bins \
            $localdata/frag_size/${base}.fragsize.tsv
    fi
done
#
for file in "${dedup_bam[@]}";
do
    base=`basename $file`
    if [ $localdata/frag_size/${base}.fragsize.tsv -ot $file ]; then
        mpnst_fragsize \
            $file \
            $processors \
            $bin_size \
            $distance_between_bins \
            $localdata/frag_size/${base}.fragsize.tsv
    fi
done
#
rm $localdata/frag_size/frag_size_summary.tsv
touch $localdata/frag_size/frag_size_summary.tsv
for file in $localdata/frag_size/*.fragsize.tsv; do
    cat $file | tail -n +3 >> $localdata/frag_size/frag_size_summary.tsv
done
#
sed -i '1 i\size\toccurences\tsample' $localdata/frag_size/frag_size_summary.tsv
#
rm $repo/data/frag_size_summary.tsv
rm $repo/data/frag_size_summary_too_big
#
summary_file_size=$(wc -c <"$localdata/frag_size/frag_size_summary.tsv")
max_size=1000000
if [ $summary_file_size -gt $max_size ]; then
    touch $repo/data/frag_size_summary_too_big
else
    cp $localdata/frag_size/frag_size_summary.tsv $repo/data/frag_size_summary.tsv
fi
#
exit
#+end_src
- Fragment size
  #+name: fragment-sampling
  #+begin_src bash
#
# Samples fragment size by TLEN in bam files
#
# Setup
exit
source ~/repos/mpnst/bin/local-setup.sh
## Variables
fragsampledir=$localdata/tmp
## Directories
rm -rf $fragsampledir
mkdir -p $fragsampledir
#
# Get lists of bam files to sample
find /localdata/box/NCI FASTQ/ -name
find /duo4/.mpnst/bam-nci/ -name "*.dedup.bam" > $fragsampledir/nci-invivo-bams
find /duo4/.mpnst/bam-nci/ -name "*.filt.sorted.bam" > $fragsampledir/nci-insilico-bams
#TODO ADD WASHU find /duo4/mpnst/

# TODO
## paramaterize sampleing count
#
# Run Setup
#
# Processes
##
#
mapfile -t nci_insilico_bams < $fragsampledir/nci-insilico-bams
for file in "${nci_insilico_bams[@]}"; do
    prebase=`basename $file`
    base="${prebase%%.*}"
    sambamba view -f sam -t 30 $file | shuf --head-count 10000 > $fragsampledir/${base}_nci_insilico_sample
done
#
#########1#########2#########3#########4#########5#########6#########7#########8
mapfile -t nci_invivo_bams < $fragsampledir/nci-invivo-bams
for file in "${nci_invivo_bams[@]}"; do
    prebase=`basename $file`
    base="${prebase%%.*}"
    sambamba view -f sam -t 30 $file | shuf --head-count 10000 > $fragsampledir/${base}_nci_invivo_sample
done

cd $fragsampledir
rm frag_concat.txt
for file in $fragsampledir/*_sample; do
    awk '{ print sqrt($9^2) "_" FILENAME }' $file >> frag_concat.txt
done
sed -i '1s/^/fragsize_\n/' frag_concat.txt
>>>>>>> 2d6bf2d62424a76f5893600fce7444a867784228

sed -i -e 's/_/,/g' frag_sum_test.txt



# find /duo4/.mpnst/fastq-washu/ -name "*HiSeqW31*R1_001_TAGC*.fastq.gz" | cut -d "_" -f 1-5
#      | parallel perl ~/repos/mpnst/bin/cp-fastq-extract-auto.pl {}\_R1_001_TAGC.fastq.gz {}\_R2_001_TAGC.fastq.gz -j 24

#+end_src

#+begin_src bash
source ./src/setup.sh
docker_interactive
jeszyman
biotools
source ~/repos/mpnst/src/setup.sh
source ~/repos/mpnst/src/functions.sh

for file in $dataDIR/bam/lib*_sub20m.bam;
do
    base=$(basename -s .bam $file)
    if [ $file -nt $dataDIR/bam/${base}_frag90_150_sorted.bam ];
    then
        frag_filter $file \
                    $dataDIR/bam \
                    90 \
                    150 \
                    40
    fi
done
#+end_src

- deeptools https://multiqc.info/docs/
- using mosdepth
  #+name: mosdepth
  #+begin_src bash
#########1#########2#########3#########4#########5#########6#########7#########8
#
### mosdepth for WGS depth calc  ###
#
# Setup
##

# Mosdepth per bam dir
##
## For deduped bams
for file in $localdata/bams/*.dedup.sorted.bam; do
    mosdepth_mpnst $file $localdata/bam-qc/dedup 250000000
done
##
#
# get simple tsv and send to repo

for file in $localdata/bam-qc/dedup/lib*.regions.bed.gz; do
    base=`basename -s .dedup.sorted.regions.bed.gz $file`
    zcat $file | awk -v FS='\t' -v var=$base 'NR <=24 {print var,$1,$4}' >> $localdata/bam-qc/dedup/all_dedup_coverage
done

header=library_id\\tchr\\tmean_coverage
sed -i "1 i$header" $localdata/bam-qc/dedup/all_dedup_coverage

## Local
>>>>>>> 2d6bf2d62424a76f5893600fce7444a867784228
source ~/repos/mpnst/bin/local-setup.sh
docker_interactive
biotools
##
## Functions
###
### Convert bams to wigs
bam_to_wig() {
    printf "Variables are: 1=bam_file 2=bam_suffix 3=outdir\n"
        base=`basename -s ${2} $1`
        if [ $3/${base}.wig -ot $1 ]; then
            /opt/hmmcopy_utils/bin/readCounter --window 1000000 --quality 20 \
                                               --chromosome "chr1,chr2,chr3,chr4,chr5,chr6,chr7,chr8,chr9,chr10,chr11,chr12,chr13,chr14,chr15,chr16,chr17,chr18,chr19,chr20,chr21,chr22,chrX,chrY" $1 > $3/${base}.wig
        fi
}
###
### Run ichor for low TF
ichor_lowfract() {
    base=`basename -s .wig $1`
    if [ $2/$base.RData -ot $1 ]; then
        Rscript /opt/ichorCNA/scripts/runIchorCNA.R \
                --id $base \
                --WIG $1 \
                --gcWig /opt/ichorCNA/inst/extdata/gc_hg19_1000kb.wig \
                --normal "c(0.95, 0.99, 0.995, 0.999)" \
                --ploidy "c(2)" \
                --maxCN 3 \
                --estimateScPrevalence FALSE \
                --scStates "c()" \
                --outDir $2
    fi
}
##
##
mkdir -p $localdata/wigs
mkdir -p $localdata/ichor
#
# Make wigs
#
#bam_to_wig /mnt/xt3/mpnst/frag-filt-bams/lib109.dedup.sorted.frag90_150.sorted.bam .dedup.sorted.frag90_150.sorted.bam $localdata/wigs
##
for file in $localdata/frag-filt-bams/lib109*.bam; do
    bam_to_wig $file \
               .dedup.sorted.frag.sorted.bam \
               $localdata/wigs
done

## For fraction-filtered WGS cfDNA
for file in $localdata/frag-filt-bams/*.bam; do
    bam_to_wig $file \
               .dedup.sorted.frag.sorted.bam \
               $localdata/wigs
done
##
## For tumor and leukocyte WGS libraries
### Make array of genomic library file paths
genomic=($(cat /drive3/users/jszymanski/repos/mpnst/data/libraries.csv | grep -e tumor -e leukocyte | grep -v "wes" | awk -F, '{print $1}' | sed 's/"//g' | sed 's/$/.dedup.sorted.bam/g' | sed 's/^/\/mnt\/xt3\/mpnst\/bams\//g'))
###
for file in ${genomic[@]}; do
    bam_to_wig $file \
               .dedup.sorted.bam \
               $localdata/wigs
done
#
##
## Send successful file list to repo
rm /drive3/users/jszymanski/repos/mpnst/data/wigs.tsv
for file in $localdata/wigs/*.wig;
do
    base=`basename -s .wig $file`
    echo $base >> /drive3/users/jszymanski/repos/mpnst/data/wigs.tsv
done
#
##RESUME HERE
# ichor
##
for file in $localdata/wigs/lib109*.wig; do
    ichor_lowfract $file $localdata/ichor
done


header=library_id\\tchr\\tmean_coverage
sed -i "1 i$header" $localdata/bam-qc/dedup/all_dedup_coverage

max_file_size=5000000
file_size=$(
    wc -c <"$localdata/bam-qc/dedup/all_dedup_coverage"
         )

if [ $filesize -gt $max_file_size ]; then
    touch $repo/data/qc/all_dedup_coverage_too_big
else
    cp $localdata/bam-qc/dedup/all_dedup_coverage $repo/qc/all_dedup_coverage.tsv
fi
#
#+end_src
  - Cant calcualte depths off [[file:~/repos/mpnst/data/bam_qc_data/mqc_mosdepth-coverage-per-contig_1.txt]] , d/n allow values under 1
  - [ ] for coverage, should intersect down to autosomes
- run and extract mosdepth
  mosdepthRAW = as_tibble(read.table(file.path(repo,"data/all_dedup_coverage.tsv"), header = T, sep = '\t', fill = TRUE))
- https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html see multiext
- ideas
  - add # # TODO setup via fastqc metrics check
    - # for read1 in $fastqdir/*_R1.fastq.gz; do
      #     base=`basename -s _R1.fastq.gz ${read1}`
      #     filesize=$(wc -c <"$bamdir/${base}.bam")
      #     if [ $minimum_bam_size -ge $filesize ]; then
      #         echo $base >> /drive3/users/jszymanski/repos/mpnst/data/small_bams
      #     fi
      # done
      # readarray -t small_bam < /drive3/users/jszymanski/repos/mpnst/data/small_bams
***** Ideas
  - filter to min file size && expected by manual spreadsheet
  - fastqs too small (< 500 Mb)
    #+begin_src bash :results replace
  find /mnt/ris/aadel/mpnst/inputs/cappseq-fastq -size -500M
  #+end_src







***** Rename                                                       :smk_rule:
- Snakemake
#+begin_src snakemake
rule rename:
    params:
        old_sample_id=lambda wcs: sampledict[wcs.f],
    output:
        read1=config["fq_symlink_dir"] + "/{f}_R1.fastq.gz",
        read2=config["fq_symlink_dir"] + "/{f}_R2.fastq.gz",
    shell:
        """
        if [ -f {output.read1} ]; then \\rm {output.read1}; fi
        if [ -f {output.read2} ]; then \\rm {output.read2}; fi
        ln -s --relative "{config[raw_fq_dir]}/{params.old_sample_id}_R1.fastq.gz" {output.read1}
        ln -s --relative "{config[raw_fq_dir]}/{params.old_sample_id}_R2.fastq.gz" {output.read2}
        """
#+end_src

**** Library QC Dataframe
#+begin_src R
library(tidyverse)

flagstat_raw = as_tibble(read.table("/home/jeszyman/repos/mpnst-preprocessing/test/qc/all_qc_data/multiqc_samtools_flagstat.txt", header = T, sep = '\t'))

flagstat_mod =
  flagstat_raw %>%
  mutate(library_id = substr(Sample, 1, 6)) %>%
  mutate(bam_type = gsub("_.*$","", gsub("^.......","",Sample))) %>%
  pivot_wider(names_from = bam_type, values_from = -c(library_id, bam_type), everything()) %>%
  select(library_id, everything(), -starts_with("Sample"))

samstats_raw = as_tibble(read.table("/home/jeszyman/repos/mpnst-preprocessing/test/qc/all_qc_data/multiqc_samtools_stats.txt", header = T, sep = '\t'))

samstats_mod =
  samstats_raw %>%
  mutate(library_id = substr(Sample, 1, 6)) %>%
  mutate(bam_type = gsub("_.*$","", gsub("^.......","",Sample))) %>%
  pivot_wider(names_from = bam_type, values_from = -c(library_id, bam_type), everything()) %>%
  select(library_id, everything(), -starts_with("Sample"))
samstats_mod

#+end_src

***** Make bwa index                                               :smk_rule:
- Snakemake
  #+begin_src snakemake
rule make_bwa_index_:
    input:
        fasta = config["data_dir"] + "/inputs/hg19.fa.gz",
    params:
        out_dir = config["data_dir"] + "/ref/hg19_bwa"
        prefix = config["data_dir"] + "/ref/hg19_bwa/hg19"
    output:
        config["data_dir"] + "/ref/hg19_bwa/hg19.amb"
    shell:
        """
        scripts/make_bwa_index_.sh {params.out_dir} {params.prefix} {input.fasta}
        """
#+end_src
- [[file:./workflow/scripts/make_bwa_index_.sh][Base script]]
  #+begin_src bash :tangle ./workflow/scripts/make_bwa_index_.sh
mkdir -p $1
bwa index -p $2 -a bwtsw $3
# Snakemake variables
# Function
# Run command
#+end_src

**** Downsample Bams
#+name: downsample_bam
#+begin_src bash
function downsample_bam {

## Calculate the sampling factor based on the intended number of reads:
FACTOR=$(samtools idxstats $1 | cut -f3 | awk -v COUNT=$2 'BEGIN {total=0} {total += $1} END {print COUNT/total}')

if [[ $FACTOR > 1 ]]; then
    echo "DS reads exceeds total for $1"
else
    sambamba view -s $FACTOR -f bam -l 5 $1
fi
}

#+end_src

#+name: downsample_bam
#+begin_src bash :tangle ./src/functions.sh
function downsample_bam {

## Calculate the sampling factor based on the intended number of reads:
FACTOR=$(samtools idxstats $1 | cut -f3 | awk -v COUNT=$2 'BEGIN {total=0} {total += $1} END {print COUNT/total}')

if [[ $FACTOR > 1 ]]; then
    echo "DS reads exceeds total for $1"
else
    sambamba view -s $FACTOR -f bam -l 5 $1
fi
}

#+end_src

** [[file:workflow/cfdna_wgs_int_test.smk][Integration testing]] :smk:
#+begin_src snakemake :tangle ./workflow/cfdna_wgs_int_test.smk
container: config["container"]

IDS, = glob_wildcards(config["fq_dir"] + "/{id}_R1.fastq.gz")
MILREADS = config["MILREADS"]

rule all:
    input:
        expand(config["processed_fq_dir"] + "/{read_id}_proc_{read}.fastq.gz", read_id = IDS, read = ["R1","R2"]),
        expand(config["unpr_fq_dir"] + "/{read_id}_unpr_R1.fastq.gz", read_id = IDS, read = ["R1","R2"]),
        expand(config["bam_dir"] + "/{read_id}.sam", read_id = IDS),
        expand(config["qc_dir"] + "/{read_id}_{read}_fastqc.html", read_id = IDS, read = ["R1","R2"]),
        expand(config["qc_dir"] + "/{read_id}_proc_{read}_fastqc.html", read_id = IDS, read = ["R1","R2"]),
        expand(config["bam_dir"] + "/{read_id}_dedup.bam", read_id = IDS),
        expand(config["bam_dir"] + "/{read_id}_dedup.bam.bai", read_id = IDS),
        expand(config["qc_dir"] + "/{read_id}_{bam_step}_samstats.txt", read_id = IDS, bam_step= ["dedup","raw"]),
        expand(config["qc_dir"] + "/{read_id}_{bam_step}_flagstat.txt", read_id = IDS, bam_step =["dedup","raw"]),
        expand(config["bam_dir"] + "/{read_id}_ds{milreads}.bam", read_id = IDS, milreads = MILREADS),
        config["qc_dir"] + "/all_qc.html",

include: "read_preprocess.smk"

rule multiqc:
    input:
        expand(config["qc_dir"] + "/{read_id}_{read}_fastqc.html", read_id = IDS, read = ["R1","R2"]),
        expand(config["qc_dir"] + "/{read_id}_proc_{read}_fastqc.html", read_id = IDS, read = ["R1","R2"]),
        expand(config["qc_dir"] + "/{read_id}_{bam_step}_samstats.txt", read_id = IDS, bam_step= ["dedup","raw"]),
        expand(config["qc_dir"] + "/{read_id}_{bam_step}_flagstat.txt", read_id = IDS, bam_step =["dedup","raw"]),
    params:
        out_dir = config["qc_dir"]
    output:
        config["qc_dir"] + "/all_qc.html"
    shell:
        """
        multiqc {params.out_dir} \
        --force \
        --outdir {params.out_dir} \
        --filename all_qc
        """

#+end_src
*** Repository-local integration testing
#+begin_src bash
src/full_repo_int_test.sh
#+end_src
*** Dev Ideas
:PROPERTIES:
:header-args:snakemake: :tangle no
:END:
#+begin_src snakemake
container: config["container"]

IDS, = glob_wildcards(config["fq_dir"] + "/{id}_R1.fastq.gz")

rule all:
    input:
        expand(config["processed_fq_dir"] + "/{read_id}_proc_{read}.fastq.gz", read_id = IDS, read = ["R1","R2"]),
        expand(config["unpr_fq_dir"] + "/{read_id}_unpr_R1.fastq.gz", read_id = IDS, read = ["R1","R2"]),
        expand(config["bam_dir"] + "/{read_id}_dedup.bam", read_id = IDS),
        expand(config["bam_dir"] + "/{read_id}_ds{milreads}.bam", read_id = IDS, milreads = config["MILREADS"]),
        expand(config["qc_dir"] + "/{read_id}_{read}_fastqc.html", read_id = IDS, read = ["R1","R2"]),
        expand(config["qc_dir"] + "/{read_id}_proc_{read}_fastqc.html", read_id = IDS, read = ["R1","R2"]),
        config["qc_dir"] + "/all_qc.html",

rule rename:
    params:
        old_sample_id=lambda wcs: sampledict[wcs.f],
    output:
        read1=config["fq_symlink_dir"] + "/{f}_R1.fastq.gz",
        read2=config["fq_symlink_dir"] + "/{f}_R2.fastq.gz",
    shell:
        """
        if [ -f {output.read1} ]; then \\rm {output.read1}; fi
        if [ -f {output.read2} ]; then \\rm {output.read2}; fi
        ln -s --relative "{config[raw_fq_dir]}/{params.old_sample_id}_R1.fastq.gz" {output.read1}
        ln -s --relative "{config[raw_fq_dir]}/{params.old_sample_id}_R2.fastq.gz" {output.read2}
        """


include: "read_preprocess.smk"
#+end_src
- Ideas
  #+begin_src snakemake
container: config["container"]
import pandas as pd
import numpy as np

samples = pd.read_table(config["inputs_dir"] + "/samples.tsv")
sampledict = dict(zip(samples['new_name'], samples['old_name']))

wildcard_constraints:
    read_id='|'.join([re.escape(x) for x in sampledict.keys()]),

rule all:
    input:
        expand(config["processed_fq_dir"] + "/{read_id}_proc_{read}.fastq.gz", read_id = sampledict.keys(), read = ["R1","R2"]),
        expand(config["unpr_fq_dir"] + "/{read_id}_unpr_R1.fastq.gz", read_id = sampledict.keys(), read = ["R1","R2"]),
	expand(config["bam_dir"] + "/{read_id}_dedup.bam", read_id = sampledict.keys()),
        config["qc_dir"] + "/all_qc.html",
        expand(config["bam_dir"] + "/{read_id}_ds{milreads}.bam", read_id = sampledict.keys(), milreads = config["MILREADS"]),

rule rename:
    params:
        old_sample_id=lambda wcs: sampledict[wcs.f],
    output:
        read1=config["fq_symlink_dir"] + "/{f}_R1.fastq.gz",
        read2=config["fq_symlink_dir"] + "/{f}_R2.fastq.gz",
    shell:
        """
        if [ -f {output.read1} ]; then \\rm {output.read1}; fi
        if [ -f {output.read2} ]; then \\rm {output.read2}; fi
        ln -s --relative "{config[raw_fq_dir]}/{params.old_sample_id}_R1.fastq.gz" {output.read1}
        ln -s --relative "{config[raw_fq_dir]}/{params.old_sample_id}_R2.fastq.gz" {output.read2}
        """


include: "read_preprocess.smk"
#+end_src
