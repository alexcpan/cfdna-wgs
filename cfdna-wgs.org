* Cell-free DNA Whole-Genome Sequencing                             :biopipe:
:PROPERTIES:
:header-args: :tangle no :tangle-mode (identity #o555)
:header-args+: :noweb yes
:logging: nil
:END:
** Setup
*** Bash preamble
#+name: bash_preamble
#+begin_src bash

#  Note: This script is tangled from code blocks in the Emacs Org-mode file at
#  https://github.com/jeszyman/cfdna-wgs/blob/master/cfdna-wgs.org. Changes
#  made directly to this file will be overwritten upon tangle from Org-mode.

#+end_src
*** [[file:config/int_test.yaml][Snakemake configuration YAML]]
:PROPERTIES:
:header-args:bash: :tangle ./config/int_test.yaml
:END:
#+begin_src bash
<#bash_preamble#>
#+end_src

#+transclude: [[id:01e0536d-4981-4e08-9ac6-9aee3e879def]] :only-contents

#+begin_src bash

##########################################################
###   Repository-local Integration Testing Variables   ###
##########################################################

cfdna_wgs_script_dir: "workflow/scripts"
picard_jar: "/opt/picard/picard.jar"
container: "~/sing_containers/biotools.sif"

# Required reference files

adapter_fastq: "test/inputs/TruSeq3-PE.fa"
genome_fasta: "test/inputs/chr8.fa"
blacklist: "test/inputs/hg38-blacklist.v2.bed"

# Required parameters
MILREADS:
  - "1"

qscore: "20"
#+end_src
*** Integration testing inputs setup
#+begin_src bash
wget --directory-prefix="/home/jeszyman/repos/cfdna-wgs/test/inputs" https://raw.githubusercontent.com/Boyle-Lab/Blacklist/master/lists/hg38-blacklist.v2.bed.gz

gunzip -c ~/repos/cfdna-wgs/test/inputs/hg38-blacklist.v2.bed.gz > ~/repos/cfdna-wgs/test/inputs/hg38-blacklist.v2.bed

ls -d1 test/* | grep -v -e inputs -e ref -e fastq

ls -d ./test/

results_dirs=test/*
results_dirs=
if [ -d test/bam]
basecamp/src/smk_forced_run.sh config/int_repo_test.yaml workflow/int_test.smk
#+end_src
#+begin_src bash
#!/bin/echo Run:.

# For documentation, not intended to be executable

if [ -d test ]; then \rm -rf test; fi
mkdir -p test/fastq
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/19_2_082_R1.fastq.gz | head -n 100000 > "test/fastq/mpnst1_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/19_2_082_R2.fastq.gz | head -n 100000 > "test/fastq/mpnst1_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/25_2_072_R1.fastq.gz | head -n 100000 > "test/fastq/mpnst2_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/25_2_072_R2.fastq.gz | head -n 100000 > "test/fastq/mpnst2_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/37_JS0050CD112717_R1.fastq.gz | head -n 100000 > "test/fastq/plex1_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/37_JS0050CD112717_R2.fastq.gz | head -n 100000 > "test/fastq/plex1_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/30_JS0044CD112818_R1.fastq.gz | head -n 100000 > "test/fastq/plex2_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/30_JS0044CD112818_R2.fastq.gz | head -n 100000 > "test/fastq/plex2_R2.fastq"
for file in "test/fastq/*.fastq"; do gzip $file; done

mkdir -p "test/inputs"
wget --directory-prefix="test/inputs/" https://raw.githubusercontent.com/usadellab/Trimmomatic/main/adapters/TruSeq3-PE.fa
wget --directory-prefix="test/inputs/" https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz

cp resources/samples.tsv test/inputs/

mkdir -p test/ref
zcat "test/inputs/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz" | grep -A 2000 chr8 > test/inputs/chr8.fa
\rm test/inputs/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz

singularity shell ~/sing_containers/biotools.sif
bwa index -p test/ref/chr8 test/inputs/chr8.fa
exit
#+end_src
** [[file:workflow/read_preprocess.smk][Sequence pre-processing, alignment, and quality control]]  :smk:
:PROPERTIES:
:header-args:snakemake: :tangle ./workflow/read_preprocess.smk
:END:
*** DONE Read pre-processing                                          :smk_rule:
- Snakemake
  #+begin_src snakemake
# Read trimming per NCI
rule trimmomatic:
    input:
        read1 = config["data_dir"] + "/fastq/raw/{library_id}_R1.fastq.gz",
        read2 = config["data_dir"] + "/fastq/raw/{library_id}_R2.fastq.gz",
    params:
        adapter_fasta = config["adapter_fastq"],
	script = config["cfdna_wgs_script_dir"] + "/trimmomatic_wrapper.sh",
    output:
        read1 = config["data_dir"] + "/fastq/processed/{library_id}_proc_R1.fastq.gz",
        read1_unpr = config["data_dir"] + "/fastq/unpaired/{library_id}_unpr_R1.fastq.gz",
        read2 = config["data_dir"] + "/fastq/processed/{library_id}_proc_R2.fastq.gz",
        read2_unpr = config["data_dir"] + "/fastq/unpaired/{library_id}_unpr_R2.fastq.gz",
    log:
        int = config["data_dir"] + "/logs/trimmomatic_trimlog_{library_id}.log",
        main = config["data_dir"] + "/logs/trimmomatic_{library_id}.log",
    shell:
        """
        {params.script} \
        {input.read1} \
        {input.read2} \
        {params.adapter_fasta} \
        {config[threads]} \
        {output.read1} \
        {output.read1_unpr} \
        {output.read2} \
        {output.read2_unpr} \
        {log.int} \
        &> {log.main}
        """
#+end_src
- [[file:workflow/scripts/trimmomatic_wrapper.sh][Script]]
  #+begin_src bash :tangle ./workflow/scripts/trimmomatic_wrapper.sh
#!/usr/bin/env bash
<#bash_preamble#>

input_read1=$1
input_read2=$2
params_adapter_fasta=$3
threads=$4
output_read1=$5
output_read1_unpr=$6
output_read2=$7
output_read2_unpr=$8
log_int=$9

trimmomatic PE \
            -threads $threads \
            -trimlog $log_int \
            $input_read1 $input_read2 \
            $output_read1 $output_read1_unpr \
            $output_read2 $output_read2_unpr \
            ILLUMINACLIP:$params_adapter_fasta:2:30:10 \
            LEADING:10 TRAILING:10 MAXINFO:50:0.97 MINLEN:20
#+end_src
- Reference
  - Trimmomatic parameters based on Taylor's parameters ([[https://mail.google.com/mail/u/0/#search/sundby+fastq/FMfcgzGmvLWSbsmhDsffvSSWfjWdQhhR?projector=1&messagePartId=0.1][email]])
  - https://github.com/AAFC-BICoE/snakemake-trimmomatic/blob/master/Snakefile
*** DONE FastQC                                                       :smk_rule:
- Snakemake
  #+begin_src snakemake
# FastQC
rule fastqc:
    input:
        raw =  config["data_dir"] + "/fastq/raw/{library_id}_{read}.fastq.gz",
        proc = config["data_dir"] + "/fastq/processed/{library_id}_proc_{read}.fastq.gz",
    params:
        out_dir = config["data_dir"] + "/qc",
    output:
        raw_html = config["data_dir"] + "/qc/{library_id}_{read}_fastqc.html",
        proc_html = config["data_dir"] + "/qc/{library_id}_proc_{read}_fastqc.html",
    log:
        raw = config["data_dir"] + "/logs/fastqc_raw_{library_id}_{read}.log",
        proc = config["data_dir"] + "/logs/fastqc_proc_{library_id}_{read}.log",
    shell:
        """
        fastqc --outdir {params.out_dir} \
        --quiet \
        --threads {config[threads]} {input.raw} &> {log}
        fastqc --outdir {params.out_dir} \
        --quiet \
        --threads {config[threads]} {input.proc} &> {log}
        """
#+end_src
*** DONE Make alignment index                                         :smk_rule:
- Snakemake
  #+begin_src snakemake
rule index:
    input:
        config["genome_fasta"],
    params:
        out_prefix = genome_ref
    output:
        done = touch(genome_ref)
    shell:
        """
        bwa index -p {params.out_prefix} {input}
        """
#+end_src
*** DONE Alignment and alignment processing                           :smk_rule:
- Snakemake
  #+begin_src snakemake
# BWA alignment
# Post-processing with samblaster and samtools
# Final bam is duplicate marked (NOT removed), location sorted
rule align:
    input:
        ref = genome_ref,
        r1 = config["data_dir"] + "/fastq/processed/{library_id}_proc_R1.fastq.gz",
        r2 = config["data_dir"] + "/fastq/processed/{library_id}_proc_R2.fastq.gz",
    params:
        script = config["cfdna_wgs_script_dir"] + "/align.sh"
    output:
        sort = config["data_dir"] + "/bam/raw/{library_id}.bam",
        index = config["data_dir"] + "/bam/raw/{library_id}.bam.bai",
    log:
        config["data_dir"] + "/logs/align_{library_id}.log"
    shell:
        """
        {params.script} \
        {input.ref} \
        {input.r1} \
        {input.r2} \
        {config[threads]} \
        {output.sort} &> {log}
	"""
#+end_src
- Shell script
  #+begin_src bash :tangle ./workflow/scripts/align.sh
#!/usr/bin/env bash

input_ref=$1
input_r1=$2
input_r2=$3
threads=$4
output_sort=$5

bwa mem -M -t $threads \
    $input_ref \
    $input_r1 \
    $input_r2 |
    samblaster -M |
    samtools view -@ $threads -Sb - -o - |
    samtools sort -@ $threads - -o $output_sort
samtools index -@ threads $output_sort
#+end_src
*** DONE Alignment QC                                                 :smk_rule:
#+begin_src snakemake
# Alignment samtools QC
rule alignment_qc:
    input:
        config["data_dir"] + "/bam/raw/{library_id}.bam",
    params:
        threads = config["threads"],
    output:
        samstat = config["data_dir"] + "/qc/{library_id}_samstats.txt",
        flagstat = config["data_dir"] + "/qc/{library_id}_flagstat.txt",
    log:
        config["data_dir"] + "/logs/alignment_qc_{library_id}.log",
    shell:
        """
        samtools stats -@ {params.threads} {input} > {output.samstat} 2>{log}
        samtools flagstat -@ {params.threads} {input} > {output.flagstat} 2>{log}
        """
#+end_src
*** DONE Alignment filtering                                          :smk_rule:
- Snakemake
  #+begin_src snakemake
# Removes unmapped, not primary, and duplicate reads. Additionally, quality filters by config variable.
rule alignment_filtering:
    input:
        config["data_dir"] + "/bam/raw/{library_id}.bam",
    params:
        script = config["cfdna_wgs_script_dir"] + "/alignment_filtering.sh",
        quality = config["qscore"],
        threads = config["threads"],
    output:
        bam = config["data_dir"] + "/bam/filt/{library_id}_filt.bam",
        bai = config["data_dir"] + "/bam/filt/{library_id}_filt.bam.bai",
    log:
        config["data_dir"] + "/logs/{library_id}_alignment_filtering.log",
    shell:
        """
        {params.script} \
        {input} \
        {params.quality} \
        {params.threads} \
        {output.bam} &> {log}
        """
#+end_src
- [[file:./workflow/scripts/alignment_filtering.sh][Shell script]]
  #+begin_src bash :tangle ./workflow/scripts/alignment_filtering.sh
#!/usr/bin/env bash

input=$1
quality=$2
threads=$3
output=$4

# Collect only deduped, mapped, paired reads of >q20
samtools idxstats ${input} | \
    cut -f 1 | \
    grep -vE 'chrM|_random|chrU|chrEBV|\*' | \
    xargs samtools view -@ $threads -f 1 -F 1284 -q $quality -o ${output} ${input}

samtools index ${output}

#+end_src
*** DONE Sequencing depth metric(s)                                   :smk_rule:
- Snakemake
  #+begin_src snakemake
# Sequencing depth via Picard
rule picard_collect_wgs_metrics:
    input:
        config["data_dir"] + "/bam/filt/{library_id}_filt.bam",
    params:
        script = config["cfdna_wgs_script_dir"] + "/CollectWgsMetrics_wrapper.sh",
    output:
        config["data_dir"] + "/qc/{library_id}_collect_wgs_metrics.txt",
    log:
        config["data_dir"] + "/logs/{library_id}_picard_wgs.log",
    shell:
        """
        {config[cfdna_wgs_script_dir]}/CollectWgsMetrics_wrapper.sh \
        {input} \
        {config[picard_jar]} \
        {config[genome_fasta]} \
        {output}
        """
#+end_src
- Script
  #+begin_src bash :tangle ./workflow/scripts/CollectWgsMetrics_wrapper.sh

input=$1
picard_jar=$2
genome_fasta=$3
output=$4

java -jar $picard_jar CollectWgsMetrics \
       INPUT=$input \
       OUTPUT=$output \
       READ_LENGTH=150 \
       REFERENCE_SEQUENCE=$genome_fasta
#+end_src

*** DONE Fragment sizes                                               :smk_rule:
- Snakemake
  #+begin_src snakemake
# Fragment sizes by deepTools
rule deeptools_bamprfragmentsize:
    input:
        config["data_dir"] + "/bam/filt/{library_id}_filt.bam",
    params:
        blacklist = config["blacklist"],
        script = config["cfdna_wgs_script_dir"] + "/bamPEFragmentSize_wrapper.sh",
    output:
        config["data_dir"] + "/qc/{library_id}_deeptools_frag_lengths.txt",
    shell:
        """
        {params.script} \
        {input} \
        {config[threads]} \
        {params[blacklist]} \
        {output}
        """
#+end_src
- Script
  #+begin_src bash :tangle ./workflow/scripts/bamPEFragmentSize_wrapper.sh
#!/usr/bin/env bash
input=$1
threads=$2
blacklist=$3
output=$4

bamPEFragmentSize --bamfiles $input \
                  --numberOfProcessors $threads \
                  --blackListFileName $blacklist \
                  --maxFragmentLength 500 \
                  --outRawFragmentLengths $output
#+end_src

  #+begin_src bash
#########1#########2#########3#########4#########5#########6#########7#########8
#
source ./src/setup.sh
docker_interactive
jeszyman
biotools
source ~/repos/mpnst/src/setup.sh

# Function
mpnst_fragsize() {
    bamPEFragmentSize --bamfiles $1 \
                      --numberOfProcessors $2 \
                      --binSize $3 \
                      --distanceBetweenBins $4 \
                      --outRawFragmentLengths $5
}

##
## Local variables
processors=40
bin_size=10000000
distance_between_bins=10000000
min_bam_size=100000000

#
# Generate bam file lists
#  Note: Small or empty bams kill bamPEFragmentSize and must be excluded
##
## For fragment-filtered bams
declare -a frag_filt_bam=()
for file in $localdata/frag-filt-bams/*.dedup.sorted.frag.sorted.bam;
do
    bamsize=$(wc -c <"$file")
    if [ $bamsize -ge $min_bam_size ]; then
        frag_filt_bam+=("$file")
    fi
done
##
## For deduped full bams
declare -a dedup_bam
for file in $localdata/bams/*.dedup.sorted.bam;
do
    bamsize=$(wc -c <"$file")
    if [ $bamsize -ge $min_bam_size ]; then
        dedup_bam+=("$file")
    fi
done
##
mkdir -p $localdata/frag_size
#
for file in "${frag_filt_bam[@]}";
do
    base=`basename $file`
    if [ $localdata/frag_size/${base}.fragsize.tsv -ot $file ]; then
        mpnst_fragsize \
            $file \
            $processors \
            $bin_size \
            $distance_between_bins \
            $localdata/frag_size/${base}.fragsize.tsv
    fi
done
#
for file in "${dedup_bam[@]}";
do
    base=`basename $file`
    if [ $localdata/frag_size/${base}.fragsize.tsv -ot $file ]; then
        mpnst_fragsize \
            $file \
            $processors \
            $bin_size \
            $distance_between_bins \
            $localdata/frag_size/${base}.fragsize.tsv
    fi
done
#
rm $localdata/frag_size/frag_size_summary.tsv
touch $localdata/frag_size/frag_size_summary.tsv
for file in $localdata/frag_size/*.fragsize.tsv; do
    cat $file | tail -n +3 >> $localdata/frag_size/frag_size_summary.tsv
done
#
sed -i '1 i\size\toccurences\tsample' $localdata/frag_size/frag_size_summary.tsv
#
rm $repo/data/frag_size_summary.tsv
rm $repo/data/frag_size_summary_too_big
#
summary_file_size=$(wc -c <"$localdata/frag_size/frag_size_summary.tsv")
max_size=1000000
if [ $summary_file_size -gt $max_size ]; then
    touch $repo/data/frag_size_summary_too_big
else
    cp $localdata/frag_size/frag_size_summary.tsv $repo/data/frag_size_summary.tsv
fi
#
exit
#+end_src
  - Fragment size
    #+name: fragment-sampling
    #+begin_src bash
  #
  # Samples fragment size by TLEN in bam files
  #
  # Setup
  exit
  source ~/repos/mpnst/bin/local-setup.sh
  ## Variables
  fragsampledir=$localdata/tmp
  ## Directories
  rm -rf $fragsampledir
  mkdir -p $fragsampledir
  #
  # Get lists of bam files to sample
  find /localdata/box/NCI FASTQ/ -name
  find /duo4/.mpnst/bam-nci/ -name "*.dedup.bam" > $fragsampledir/nci-invivo-bams
  find /duo4/.mpnst/bam-nci/ -name "*.filt.sorted.bam" > $fragsampledir/nci-insilico-bams
  #TODO ADD WASHU find /duo4/mpnst/

  # TODO
  ## paramaterize sampleing count
  #
  # Run Setup
  #
  # Processes
  ##
  #
  mapfile -t nci_insilico_bams < $fragsampledir/nci-insilico-bams
  for file in "${nci_insilico_bams[@]}"; do
      prebase=`basename $file`
      base="${prebase%%.*}"
      sambamba view -f sam -t 30 $file | shuf --head-count 10000 > $fragsampledir/${base}_nci_insilico_sample
  done
  #
  #########1#########2#########3#########4#########5#########6#########7#########8
  mapfile -t nci_invivo_bams < $fragsampledir/nci-invivo-bams
  for file in "${nci_invivo_bams[@]}"; do
      prebase=`basename $file`
      base="${prebase%%.*}"
      sambamba view -f sam -t 30 $file | shuf --head-count 10000 > $fragsampledir/${base}_nci_invivo_sample
  done

  cd $fragsampledir
  rm frag_concat.txt
  for file in $fragsampledir/*_sample; do
      awk '{ print sqrt($9^2) "_" FILENAME }' $file >> frag_concat.txt
  done
  sed -i '1s/^/fragsize_\n/' frag_concat.txt
  >>>>>>> 2d6bf2d62424a76f5893600fce7444a867784228

  sed -i -e 's/_/,/g' frag_sum_test.txt



  # find /duo4/.mpnst/fastq-washu/ -name "*HiSeqW31*R1_001_TAGC*.fastq.gz" | cut -d "_" -f 1-5
  #      | parallel perl ~/repos/mpnst/bin/cp-fastq-extract-auto.pl {}\_R1_001_TAGC.fastq.gz {}\_R2_001_TAGC.fastq.gz -j 24

  #+end_src
    #+begin_src bash
  source ./src/setup.sh
  docker_interactive
  jeszyman
  biotools
  source ~/repos/mpnst/src/setup.sh
  source ~/repos/mpnst/src/functions.sh

  for file in $dataDIR/bam/lib*_sub20m.bam;
  do
      base=$(basename -s .bam $file)
      if [ $file -nt $dataDIR/bam/${base}_frag90_150_sorted.bam ];
      then
          frag_filter $file \
                      $dataDIR/bam \
                      90 \
                      150 \
                      40
      fi
  done
  #+end_src
*** DONE Multiqc                                                      :smk_rule:
- Snakemake
  #+begin_src snakemake
rule multiqc:
    input:
        expand(config["data_dir"] + "/qc/{library_id}_{read}_fastqc.html", library_id = LIBRARY_IDS, read = ["R1","R2"]),
        expand(config["data_dir"] + "/qc/{library_id}_proc_{read}_fastqc.html", library_id = LIBRARY_IDS, read = ["R1","R2"]),
        expand(config["data_dir"] + "/qc/{library_id}_samstats.txt", library_id = LIBRARY_IDS),
        expand(config["data_dir"] + "/qc/{library_id}_flagstat.txt", library_id = LIBRARY_IDS),
        expand(config["data_dir"] + "/qc/{library_id}_deeptools_frag_lengths.txt", library_id = LIBRARY_IDS),
        expand(config["data_dir"] + "/qc/{library_id}_deeptools_frag_lengths.txt", library_id = LIBRARY_IDS),
        expand(config["data_dir"] + "/qc/{library_id}_collect_wgs_metrics.txt", library_id = LIBRARY_IDS),
    params:
        out_dir = config["data_dir"] + "/qc"
    output:
        config["data_dir"] + "/qc/all_qc_data/multiqc_fastqc.txt",
        config["data_dir"] + "/qc/all_qc_data/multiqc_samtools_stats.txt",
        config["data_dir"] + "/qc/all_qc_data/multiqc_samtools_flagstat.txt",
	config["data_dir"] + "/qc/all_qc_data/multiqc_picard_wgsmetrics.txt",
    shell:
        """
        multiqc {params.out_dir} \
        --force \
        --outdir {params.out_dir} \
        --filename all_qc
        """
#+end_src
*** DONE Make aggregate fragment table                                :smk_rule:
- Snakemake
  #+begin_src snakemake
rule aggregate_frag:
    input:
        expand(config["data_dir"] + "/qc/{library_id}_deeptools_frag_lengths.txt", library_id = LIBRARY_IDS),
    params:
        script = config["cfdna_wgs_script_dir"] + "/aggregate_frag.sh",
    output:
        config["data_dir"] + "/qc/all_frag.tsv",
    log:
        config["data_dir"] + "/logs/aggregate_frag.err",
    shell:
        """
        awk 'FNR>2' {input} > {output} 2> {log}
        """
#+end_src
- [[file:./workflow/scripts/aggregate_frag.sh][Shell script]]
  #+begin_src bash :tangle ./workflow/scripts/aggregate_frag.sh
#!/usr/bin/env bash
input=$1
output=$2

cat $input > $output
#+end_src
*** DONE Make QC table                                                :smk_rule:
- Snakemake
  #+begin_src snakemake
#  Notes:
#  This makes an aggregate table of QC values. The subsequent downsampling
#  step only runs if read numbers are above a certain threshold. See also
#  the int_test.smk for function using this output table.
#

checkpoint make_qc_tbl:
    input:
        fq = config["data_dir"] + "/qc/all_qc_data/multiqc_fastqc.txt",
        sam = config["data_dir"] + "/qc/all_qc_data/multiqc_samtools_stats.txt",
        flag = config["data_dir"] + "/qc/all_qc_data/multiqc_samtools_flagstat.txt",
	picard = config["data_dir"] + "/qc/all_qc_data/multiqc_picard_wgsmetrics.txt",
        deeptools = config["data_dir"] + "/qc/all_frag.tsv",
    params:
        script = config["cfdna_wgs_script_dir"] + "/make_qc_tbl.R"
    output:
        config["data_dir"] + "/qc/read_qc.tsv",
    log:
        config["data_dir"] + "/logs/read_qc.log"
    shell:
        """
        Rscript {params.script} \
        {input.fq} \
        {input.sam} \
        {input.flag} \
        {input.picard} \
        {input.deeptools} \
        {output} \
        >& {log}
        """
#+end_src
- Rscript
  #+begin_src R
#fastqc_input="test/qc/all_qc_data/multiqc_fastqc.txt"
#samstats_input="test/qc/all_qc_data/multiqc_samtools_stats.txt"
#flagstats_input="test/qc/all_qc_data/multiqc_samtools_flagstat.txt"

args = commandArgs(trailingOnly = TRUE)
fastqc_input = args[1]
samstats_input = args[2]
flagstats_input = args[3]
readqc_out_tbl = args[4]

library(tidyverse)

fastqc = as_tibble(read.table(fastqc_input, header = TRUE, sep = '\t', stringsAsFactors = FALSE)) %>%
  mutate(library = substr(Filename,1,6)) %>%
  mutate(read = ifelse(grepl("R1", Filename), "read1", "read2")) %>%
  mutate(fastq_processing = ifelse(grepl("proc", Filename), "processed", "raw")) %>%
  select(!c(Sample,File.type,Encoding)) %>%
  pivot_wider(
    names_from = c(read,fastq_processing),
    values_from = !c(library,read,fastq_processing))

samstats = as_tibble(read.table(samstats_input, header = TRUE, sep = '\t', stringsAsFactors = FALSE)) %>%
  mutate(library = substr(Sample, 1, 6)) %>%
  mutate(bam_processing = ifelse(grepl("dedup",Sample), "dedup", "raw")) %>%
  pivot_wider(
    names_from = bam_processing,
    values_from = !c(library, bam_processing))

flagstats = as_tibble(read.table(flagstats_input, header = TRUE, sep = '\t', stringsAsFactors = FALSE)) %>%
  mutate(library = substr(Sample, 1, 6)) %>%
  mutate(bam_processing = ifelse(grepl("dedup",Sample), "dedup", "raw")) %>%
  pivot_wider(
    names_from = bam_processing,
    values_from = !c(library, bam_processing))

readqc = fastqc %>% left_join(samstats, by = "library") %>% left_join(flagstats, by = "library")

write.table(readqc, file = readqc_out_tbl, row.names = F, sep = '\t', quote = F)
#+end_src
- Dev
  #+begin_src R :tangle ./workflow/scripts/make_qc_tbl.R
## fastqc_input="test/qc/all_qc_data/multiqc_fastqc.txt"
## samstats_input="test/qc/all_qc_data/multiqc_samtools_stats.txt"
## flagstats_input="test/qc/all_qc_data/multiqc_samtools_flagstat.txt"
## picard_input="test/qc/all_qc_data/multiqc_picard_wgsmetrics.txt"
## deeptools_input="test/qc/all_frag.tsv"

args = commandArgs(trailingOnly = TRUE)
fastqc_input = args[1]
samstats_input = args[2]
flagstats_input = args[3]
picard_input = args[4]
deeptools_input = args[5]
readqc_out_tbl = args[6]

library(tidyverse)

fastqc = as_tibble(read.table(fastqc_input, header = TRUE, sep = '\t', stringsAsFactors = FALSE)) %>%
  mutate(library = substr(Filename,1,6)) %>%
  mutate(read = ifelse(grepl("R1", Filename), "read1", "read2")) %>%
  mutate(fastq_processing = ifelse(grepl("proc", Filename), "processed", "raw")) %>%
  select(!c(Sample,File.type,Encoding)) %>%
  pivot_wider(
    names_from = c(read,fastq_processing),
    values_from = !c(library,read,fastq_processing))

samstats = as_tibble(read.table(samstats_input, header = TRUE, sep = '\t', stringsAsFactors = FALSE)) %>%
  mutate(library = substr(Sample, 1, 6))

flagstats = as_tibble(read.table(flagstats_input, header = TRUE, sep = '\t', stringsAsFactors = FALSE)) %>%
  mutate(library = substr(Sample, 1, 6))

deeptools = as_tibble(read.table(deeptools_input, header = FALSE, sep = '\t', stringsAsFactors = FALSE))
colnames(deeptools)=c("frag_len","frag_count","file")
deeptools = deeptools %>%
  mutate(library = substr(file, nchar(file) -9, nchar(file) -4)) %>%
  mutate(frag_len = sub("^", "frag_len", frag_len)) %>%
  select(library, frag_len, frag_count) %>%
  pivot_wider(
    names_from = frag_len,
    values_from = frag_count)

picard = as_tibble(read.table(picard_input, header = TRUE, sep = '\t', stringsAsFactors = FALSE)) %>%
  mutate(library = Sample)

readqc = fastqc %>%
  left_join(samstats, by = "library") %>%
  left_join(flagstats, by = "library") %>%
  left_join(deeptools, by = "library") %>%
  left_join(picard, by = "library")

write.table(readqc, file = readqc_out_tbl, row.names = F, sep = '\t', quote = F)
#+end_src

*** TODO Downsample bams                                           :smk_rule:
- Snakemake
  #+begin_src snakemake
# Alignment downsampling
#  Note: Used for all rule input "get_ds_candidates". See that function in
#  workflow/int_test.smk

rule downsample_bams:
    input:
        config["data_dir"] + "/bam/filt/{library_id}_filt.bam",
    output:
        config["data_dir"] + "/bam/ds/{library_id}_ds{milreads}.bam",
    log:
        config["data_dir"] + "/logs/downsample_bam_{library_id}_{milreads}.err"
    shell:
        """
        {config[cfdna_wgs_script_dir]}/downsample_bam.sh {input} {wildcards.milreads} {output} 2>{log}
        """
#+end_src
- Shell script
  #+begin_src bash :tangle ./workflow/scripts/downsample_bam.sh
## Calculate the sampling factor based on the intended number of reads:
FACTOR=$(samtools idxstats $1 | cut -f3 | awk -v COUNT=$2 'BEGIN {total=0} {total += $1} END {print COUNT/total}')

if [[ $FACTOR > 1 ]]; then
    echo "DS reads exceeds total for $1"
    cp $1 $3
else
    sambamba view -s $FACTOR -f bam -l 5 $1 > $3
fi
#+end_src
#+begin_src bash
# Collect only deduped, mapped, paired reads of >q20
samtools idxstats test/bam/lib001.bam | cut -f 1 | grep -vE 'chrM|_random|chrU|chrEBV|\*' | \
xargs samtools view -f 1 -F 1284 -q 20 -o /tmp/test.bam test/bam/lib001.bam

# From this high-quality subset, perform downsampling to a set number of reads:
FACTOR=$(samtools idxstats $1 | cut -f3 | awk -v COUNT=$2 'BEGIN {total=0} {total += $1} END {print COUNT/total}')

if [[ $FACTOR > 1 ]]; then
    echo "DS reads exceeds total for $1"
else
samtools idxstats in.bam | cut -f 1 | grep -vE 'chrM|_random|chrU|chrEBV|\*' | \
xargs samtools view -f 1 -F 1284 -q 20 -o out.bam in.bam
    sambamba view -s $FACTOR -f bam -l 5 $1 > $3
fi
#+end_src
*** Hide
:PROPERTIES:
:header-args:snakemake: :tangle no
:END:
** [[file:workflow/cfdna_wgs_int_test.smk][Integration testing]]                                                  :smk:
*** snakefile
:PROPERTIES:
:header-args:snakemake: :tangle ./workflow/int_test.smk
:END:
- Preamble, variable naming and functions
  #+begin_src snakemake

##################################################################
###   Integration testing snakefile for WGS cfDNA Processing   ###
##################################################################

import pandas as pd
import re
container: config["container"]

# Import libraries table to pandas
libraries = pd.read_table(config["data_dir"] + "/inputs/libraries.tsv")

# Pull library ids out of libraries table
LIBRARY_IDS = list(libraries.library.unique())

# List of downsampling values in millions of reads
MILREADS = config["MILREADS"]


# Function acts on read_qc, generated in the workflow, to select libraries for
# downsampling. Notice library 2 does not downsample because it already has
# fewer than 3000 reads. Best practice for real data would be to use the
# MILREADS value in lieu of a specified number here.

def get_ds_candidates(wildcards):
    read_qc = pd.read_table(checkpoints.make_qc_tbl.get().output[0])
    test=read_qc.library[read_qc.reads_properly_paired > 3000].tolist()
    return expand(
	config["data_dir"] + "/bam/ds/{library_id}_ds{milreads}.bam",
        library_id=test, milreads = MILREADS)

# Makes the name bwa index directory from the config genome fasta
#  e.g. test/inputs/chr8.fa will make test/ref/chr8
genome_ref = config["genome_fasta"]
genome_ref = re.sub("inputs", lambda x: 'ref', genome_ref)
genome_ref = re.sub("\..*$", lambda x: '', genome_ref)

#+end_src
- All rule and other out-of-workflow rules
  #+begin_src snakemake

#########1#########2#########3#########4#########5#########6#########7#########8

rule all:
    input:
        #expand(config["data_dir"] + "/fastq/processed/{library_id}_proc_R1.fastq.gz", library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/fastq/unpaired/{library_id}_unpr_R1.fastq.gz", library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/fastq/processed/{library_id}_proc_R2.fastq.gz", library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/fastq/unpaired/{library_id}_unpr_R2.fastq.gz", library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/qc/{library_id}_{read}_fastqc.html", library_id = LIBRARY_IDS, read = ["R1","R2"]),
        #expand(config["data_dir"] + "/qc/{library_id}_proc_{read}_fastqc.html", library_id = LIBRARY_IDS, read = ["R1","R2"]),
        #expand(config["data_dir"] + "/bam/raw/{library_id}.bam", library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/bam/raw/{library_id}.bam.bai", library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/bam/filt/{library_id}_filt.bam",	library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/bam/filt/{library_id}_filt.bam.bai", library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/qc/{library_id}_samstats.txt", library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/qc/{library_id}_flagstat.txt", library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/qc/{library_id}_collect_wgs_metrics.txt", library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/qc/{library_id}_deeptools_frag_lengths.txt", library_id = LIBRARY_IDS),
        #config["data_dir"] + "/qc/all_frag.tsv",
        #
        config["data_dir"] + "/qc/read_qc.tsv",
        get_ds_candidates,

rule symlink:
    input:
        config["data_dir"] + "/inputs/{library_id}_{read}.fastq.gz",
    output:
        config["data_dir"] + "/fastq/raw/{library_id}_{read}.fastq.gz",
    log:
        config["data_dir"] + "/logs/{library_id}_{read}_symlink.log"
    shell:
        """
        ln --force --relative --symbolic {input} {output} 2>{log}
        """

include: "read_preprocess.smk"

#+end_src
*** shell scripts
#+begin_src bash :tangle ./tools/shell/rm_outputs.sh
#!/usr/bin/env bash

shopt -s extglob
cd ./test
rm -rf !(inputs)
cd ../

#+end_src
#+begin_src bash :tangle ./tools/shell/int_test.sh
#!/usr/bin/env bash
shopt -s extglob
cd test
\rm -rf !(inputs)
cd ../

smk_dry_run.sh config/int_test.yaml workflow/int_test.smk \
    && smk_draw.sh config/int_test.yaml workflow/int_test.smk resources/int_test.pdf \
    && smk_forced_run.sh config/int_test.yaml workflow/int_test.smk \
    && echo "Integration testing passed, do you want to erase results files?" \
    && select yn in "Yes" "No"; do
           case $yn in
               Yes )
                   shopt -s extglob
                   cd test
                   \rm -rf !(inputs)
                   cd ../; break;;
               No ) exit;;
           esac
       done

#+end_src
*** [[file:resources/int_test.pdf]]

** [[file:README.md][README]]
:PROPERTIES:
:export_file_name: ./README.md
:export_options: toc:nil ^:nil
:END:
*** Introduction
This repository has a snakemake workflow for basic processing of whole-genome sequencing reads from cell-free DNA.

[[file:resources/int_test.png]]

Master branch of the repository contains most recent developments. Stable versions are saved as terminal branches (/e.g./ stable1.0.0).

Files labeled int_test will run integration testing of all rules on a small dataset in test/inputs. See config/int_test.yaml for necessary run conditions.


*** Changlog
- [2022-06-27 Mon] - Version 4 validated. Further expanded read_qc.tsv table. Removed bam post-processing step and added a more expansive bam filtering step. Updated downsampling to work off filtered alignments.
- [2022-06-26 Sun] - Version 3.2 validated. Expanded the qc aggregate table and added some comments.
- [2022-06-24 Fri] - Validate version 3.1 which includes genome index build as a snakefile rule.
- [2022-06-24 Fri] - Validated version 3 with read number checkpoint for down-sampling.
- [2022-05-31 Tue] - Conforms to current biotools best practices.
- [2022-04-29 Fri] - Moved multiqc to integration testing as inputs are dependent on final sample labels. Integration testing works per this commit.
** Dev
:PROPERTIES:
:header-args:snakemake: :tangle no
:END:
- Prioritized [2022-06-07 Tue]
  1. Library QC Dataframe
  2. Mem cap and performance workup
  3. deeptools integration to multiqc https://multiqc.info/docs/
  4. fastqs too small (< 500 Mb)
     #+begin_src bash :results replace
   find /mnt/ris/aadel/mpnst/inputs/cappseq-fastq -size -500M
   #+end_src
  5. https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html see multiext
- expand seq depth metrics
  - https://gatk.broadinstitute.org/hc/en-us/articles/360037226132-CollectWgsMetrics-Picard-
  - https://github.com/brentp/mosdepth
  - https://deeptools.readthedocs.io/en/develop/content/tools/bamCoverage.html#Output
  - using mosdepth
    #+name: mosdepth
    #+begin_src bash
  #########1#########2#########3#########4#########5#########6#########7#########8
  #
  ### mosdepth for WGS depth calc  ###
  #
  # Setup
  ##

  # Mosdepth per bam dir
  ##
  ## For deduped bams
  for file in $localdata/bams/*.dedup.sorted.bam; do
      mosdepth_mpnst $file $localdata/bam-qc/dedup 250000000
  done
  ##
  #
  # get simple tsv and send to repo

  for file in $localdata/bam-qc/dedup/lib*.regions.bed.gz; do
      base=`basename -s .dedup.sorted.regions.bed.gz $file`
      zcat $file | awk -v FS='\t' -v var=$base 'NR <=24 {print var,$1,$4}' >> $localdata/bam-qc/dedup/all_dedup_coverage
  done

  header=library_id\\tchr\\tmean_coverage
  sed -i "1 i$header" $localdata/bam-qc/dedup/all_dedup_coverage

  ## Local
  >>>>>>> 2d6bf2d62424a76f5893600fce7444a867784228
  source ~/repos/mpnst/bin/local-setup.sh
  docker_interactive
  biotools
  ##
  ## Functions
  ###
  ### Convert bams to wigs
  bam_to_wig() {
      printf "Variables are: 1=bam_file 2=bam_suffix 3=outdir\n"
          base=`basename -s ${2} $1`
          if [ $3/${base}.wig -ot $1 ]; then
              /opt/hmmcopy_utils/bin/readCounter --window 1000000 --quality 20 \
                                                 --chromosome "chr1,chr2,chr3,chr4,chr5,chr6,chr7,chr8,chr9,chr10,chr11,chr12,chr13,chr14,chr15,chr16,chr17,chr18,chr19,chr20,chr21,chr22,chrX,chrY" $1 > $3/${base}.wig
          fi
  }
  ###
  ### Run ichor for low TF
  ichor_lowfract() {
      base=`basename -s .wig $1`
      if [ $2/$base.RData -ot $1 ]; then
          Rscript /opt/ichorCNA/scripts/runIchorCNA.R \
                  --id $base \
                  --WIG $1 \
                  --gcWig /opt/ichorCNA/inst/extdata/gc_hg19_1000kb.wig \
                  --normal "c(0.95, 0.99, 0.995, 0.999)" \
                  --ploidy "c(2)" \
                  --maxCN 3 \
                  --estimateScPrevalence FALSE \
                  --scStates "c()" \
                  --outDir $2
      fi
  }
  ##
  ##
  mkdir -p $localdata/wigs
  mkdir -p $localdata/ichor
  #
  # Make wigs
  #
  #bam_to_wig /mnt/xt3/mpnst/frag-filt-bams/lib109.dedup.sorted.frag90_150.sorted.bam .dedup.sorted.frag90_150.sorted.bam $localdata/wigs
  ##
  for file in $localdata/frag-filt-bams/lib109*.bam; do
      bam_to_wig $file \
                 .dedup.sorted.frag.sorted.bam \
                 $localdata/wigs
  done

  ## For fraction-filtered WGS cfDNA
  for file in $localdata/frag-filt-bams/*.bam; do
      bam_to_wig $file \
                 .dedup.sorted.frag.sorted.bam \
                 $localdata/wigs
  done
  ##
  ## For tumor and leukocyte WGS libraries
  ### Make array of genomic library file paths
  genomic=($(cat /drive3/users/jszymanski/repos/mpnst/data/libraries.csv | grep -e tumor -e leukocyte | grep -v "wes" | awk -F, '{print $1}' | sed 's/"//g' | sed 's/$/.dedup.sorted.bam/g' | sed 's/^/\/mnt\/xt3\/mpnst\/bams\//g'))
  ###
  for file in ${genomic[@]}; do
      bam_to_wig $file \
                 .dedup.sorted.bam \
                 $localdata/wigs
  done
  #
  ##
  ## Send successful file list to repo
  rm /drive3/users/jszymanski/repos/mpnst/data/wigs.tsv
  for file in $localdata/wigs/*.wig;
  do
      base=`basename -s .wig $file`
      echo $base >> /drive3/users/jszymanski/repos/mpnst/data/wigs.tsv
  done
  #
  ##RESUME HERE
  # ichor
  ##
  for file in $localdata/wigs/lib109*.wig; do
      ichor_lowfract $file $localdata/ichor
  done


  header=library_id\\tchr\\tmean_coverage
  sed -i "1 i$header" $localdata/bam-qc/dedup/all_dedup_coverage

  max_file_size=5000000
  file_size=$(
      wc -c <"$localdata/bam-qc/dedup/all_dedup_coverage"
           )

  if [ $filesize -gt $max_file_size ]; then
      touch $repo/data/qc/all_dedup_coverage_too_big
  else
      cp $localdata/bam-qc/dedup/all_dedup_coverage $repo/qc/all_dedup_coverage.tsv
  fi
  #
  #+end_src
    - Cant calcualte depths off [[file:~/repos/mpnst/data/bam_qc_data/mqc_mosdepth-coverage-per-contig_1.txt]] , d/n allow values under 1
    - [ ] for coverage, should intersect down to autosomes
  - run and extract mosdepth
    mosdepthRAW = as_tibble(read.table(file.path(repo,"data/all_dedup_coverage.tsv"), header = T, sep = '\t', fill = TRUE))
*** Old rules
**** DONE Alignment processing                                     :smk_rule:
#+begin_src snakemake
# Alignment deduplication and sorting
rule alignment_processing:
    input:
        config["data_dir"] + "/bam/{library_id}_raw.bam",
    output:
        dedup = temp(config["data_dir"] + "/bam/{library_id}_dedup_unsort.bam"),
        sort = config["data_dir"] + "/bam/{library_id}_dedup.bam",
        index = config["data_dir"] + "/bam/{library_id}_dedup.bam.bai",
    log:
        config["data_dir"] + "/logs/alignment_processing_{library_id}.log"
    shell:
        """
        {config[cfdna_wgs_script_dir]}/alignment_processing.sh \
        {input} \
        {config[threads]} \
        {output.bam} \
        {output.dedup} \
        {output.sort} \
        {output.index} \
        &> {log}
        """
#+end_src
- [[file:workflow/scripts/alignment_processing.sh][Script]]
  #+begin_src bash :tangle ./workflow/scripts/alignment_processing.sh
#!/usr/bin/env bash

<#bash_preamble#>

input=$1
threads=$2
output_bam=$3
output_dedup=$4
output_sort=$5
output_index=$6

sambamba view -t $threads -S -f bam $input > $output_bam
sambamba markdup -r -t $threads $output_bam $output_dedup
sambamba sort -t $threads $output_dedup -o $output_sort
sambamba index -t $threads $output_sort

#+end_src
** Reference
- https://github.com/jeszyman/cfdna-wgs
- [[id:271b4d5f-727e-496e-b835-8fe9f8655655][Bioinformatics project module]]
*** [[id:13120759-71db-497c-8ed3-1c58e47a7840][Biotools headline]]
