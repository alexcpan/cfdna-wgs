- ...rebuild local sing and add checker
- [ ] sing builds to box
- qc summary and checkpoint

* Cell-free DNA Whole-Genome Sequencing                             :biopipe:
:PROPERTIES:
:header-args: :tangle no :tangle-mode (identity #o555)
:logging: nil
:END:
** Setup
*** Snakemake configuration YAML
#+begin_src bash :tangle ./config/int_test.yaml
# Integration testing is on software in the jeszyman/biotools dockerfile.
#container: "~/sing_containers/biotools.sif"
container: "~/sing_containers/mpnst.sif"

# Required paths
#  (Data directory will contain all snakefile outputs in a sub-directory.)
data_dir: "test"
cfdna_wgs_script_dir: "workflow/scripts"

# Required reference files
adapter_fastq: "test/inputs/TruSeq3-PE.fa"
genome_fasta: "test/inputs/chr8.fa"
bwa_index: "test/inputs/chr8"
blacklist: "test/inputs/hg38-blacklist.v2.bed"

# Required parameters
threads: 4
MILREADS:
  - "10"
  - "20"

#+end_src
*** Integration testing inputs setup
#+begin_src bash
wget --directory-prefix="/home/jeszyman/repos/cfdna-wgs/test/inputs" https://raw.githubusercontent.com/Boyle-Lab/Blacklist/master/lists/hg38-blacklist.v2.bed.gz

gunzip -c ~/repos/cfdna-wgs/test/inputs/hg38-blacklist.v2.bed.gz > ~/repos/cfdna-wgs/test/inputs/hg38-blacklist.v2.bed

ls -d1 test/* | grep -v -e inputs -e ref -e fastq

ls -d ./test/

results_dirs=test/*
results_dirs=
if [ -d test/bam]
basecamp/src/smk_forced_run.sh config/int_repo_test.yaml workflow/int_test.smk
#+end_src
#+begin_src bash
#!/bin/echo Run:.

# For documentation, not intended to be executable

if [ -d test ]; then \rm -rf test; fi
mkdir -p test/fastq
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/19_2_082_R1.fastq.gz | head -n 100000 > "test/fastq/mpnst1_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/19_2_082_R2.fastq.gz | head -n 100000 > "test/fastq/mpnst1_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/25_2_072_R1.fastq.gz | head -n 100000 > "test/fastq/mpnst2_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/MPNST/25_2_072_R2.fastq.gz | head -n 100000 > "test/fastq/mpnst2_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/37_JS0050CD112717_R1.fastq.gz | head -n 100000 > "test/fastq/plex1_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/37_JS0050CD112717_R2.fastq.gz | head -n 100000 > "test/fastq/plex1_R2.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/30_JS0044CD112818_R1.fastq.gz | head -n 100000 > "test/fastq/plex2_R1.fastq"
zcat /mnt/ris/aadel/mpnst/inputs/PN/30_JS0044CD112818_R2.fastq.gz | head -n 100000 > "test/fastq/plex2_R2.fastq"
for file in "test/fastq/*.fastq"; do gzip $file; done

mkdir -p "test/inputs"
wget --directory-prefix="test/inputs/" https://raw.githubusercontent.com/usadellab/Trimmomatic/main/adapters/TruSeq3-PE.fa
wget --directory-prefix="test/inputs/" https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz

cp resources/samples.tsv test/inputs/

mkdir -p test/ref
zcat "test/inputs/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz" | grep -A 2000 chr8 > test/inputs/chr8.fa
\rm test/inputs/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz

singularity shell ~/sing_containers/biotools.sif
bwa index -p test/ref/chr8 test/inputs/chr8.fa
exit
#+end_src
** [[file:workflow/read_preprocess.smk][Sequence pre-processing, alignment, and quality control]]  :smk:
:PROPERTIES:
:header-args:snakemake: :tangle ./workflow/read_preprocess.smk
:END:
*** DONE Read pre-processing                                          :smk_rule:
- Snakemake
  #+begin_src snakemake
# Read trimming per NCI
rule trimmomatic:
    input:
        read1 = config["data_dir"] + "/fastq/raw/{library_id}_R1.fastq.gz",
        read2 = config["data_dir"] + "/fastq/raw/{library_id}_R2.fastq.gz",
    params:
        adapter_fasta = config["adapter_fastq"],
    output:
        read1 = config["data_dir"] + "/fastq/processed/{library_id}_proc_R1.fastq.gz",
        read1_unpr = config["data_dir"] + "/fastq/unpaired/{library_id}_unpr_R1.fastq.gz",
        read2 = config["data_dir"] + "/fastq/processed/{library_id}_proc_R2.fastq.gz",
        read2_unpr = config["data_dir"] + "/fastq/unpaired/{library_id}_unpr_R2.fastq.gz",
    log:
        int = config["data_dir"] + "/logs/trimmomatic_trimlog_{library_id}.log",
        main = config["data_dir"] + "/logs/trimmomatic_{library_id}.log",
    shell:
        """
        trimmomatic PE \
                    -threads {config[threads]} \
                    -trimlog {log.int} \
                    {input.read1} {input.read2} \
                    {output.read1} {output.read1_unpr} \
                    {output.read2} {output.read2_unpr} \
                    ILLUMINACLIP:{params.adapter_fasta}:2:30:10 \
                    LEADING:10 TRAILING:10 MAXINFO:50:0.97 MINLEN:20 &> {log.main}
        """
#+end_src
- Reference
  - Trimmomatic parameters based on Taylor's parameters ([[https://mail.google.com/mail/u/0/#search/sundby+fastq/FMfcgzGmvLWSbsmhDsffvSSWfjWdQhhR?projector=1&messagePartId=0.1][email]])
  - https://github.com/AAFC-BICoE/snakemake-trimmomatic/blob/master/Snakefile
*** DONE Alignment :smk_rule:
#+begin_src snakemake
# BWA alignment
rule align:
    input:
        read1 = config["data_dir"] + "/fastq/processed/{library_id}_proc_R1.fastq.gz",
        read2 = config["data_dir"] + "/fastq/processed/{library_id}_proc_R2.fastq.gz",
    output:
        config["data_dir"] + "/bam/{library_id}.sam",
    log:
        config["data_dir"] + "/logs/align_{library_id}.log"
    shell:
        """
        bwa mem -M -t 4 {config[bwa_index]} {input.read1} {input.read2} > {output}
	"""
#+end_src

*** DONE FastQC                                                       :smk_rule:
- Snakemake
  #+begin_src snakemake
# FastQC
rule fastqc:
    input:
        raw =  config["data_dir"] + "/fastq/raw/{library_id}_{read}.fastq.gz",
        proc = config["data_dir"] + "/fastq/processed/{library_id}_proc_{read}.fastq.gz",
    params:
        out_dir = config["data_dir"] + "/qc",
    output:
        raw_html = config["data_dir"] + "/qc/{library_id}_{read}_fastqc.html",
        proc_html = config["data_dir"] + "/qc/{library_id}_proc_{read}_fastqc.html",
    log:
        raw = config["data_dir"] + "/logs/fastqc_raw_{library_id}_{read}.log",
        proc = config["data_dir"] + "/logs/fastqc_proc_{library_id}_{read}.log",
    shell:
        """
        fastqc --outdir {params.out_dir} \
        --quiet \
        --threads {config[threads]} {input.raw} &> {log}
        fastqc --outdir {params.out_dir} \
        --quiet \
        --threads {config[threads]} {input.proc} &> {log}
        """
#+end_src
*** DONE Alignment processing :smk_rule:
#+begin_src snakemake
# Alignment deduplication and sorting
rule alignment_processing:
    input:
        config["data_dir"] + "/bam/{library_id}.sam",
    output:
        bam = config["data_dir"] + "/bam/{library_id}_raw.bam",
        dedup = temp(config["data_dir"] + "/bam/{library_id}_dedup_unsort.bam"),
        sort = config["data_dir"] + "/bam/{library_id}_dedup.bam",
        index = config["data_dir"] + "/bam/{library_id}_dedup.bam.bai",
    log:
        config["data_dir"] + "/logs/alignment_processing_{library_id}.log"
    shell:
        """
        sambamba view -t {config[threads]} -S -f bam {input} > {output.bam}
        sambamba markdup -r -t {config[threads]} {output.bam} {output.dedup}
        sambamba sort -t {config[threads]} {output.dedup} -o {output.sort}
        sambamba index -t {config[threads]} {output.sort}
        """
#+end_src
*** DONE Alignment QC :smk_rule:
#+begin_src snakemake
# Alignment samtools QC
rule alignment_qc:
    input:
        config["data_dir"] + "/bam/{library_id}_{bam_step}.bam",
    output:
        samstat = config["data_dir"] + "/qc/{library_id}_{bam_step}_samstats.txt",
        flagstat = config["data_dir"] + "/qc/{library_id}_{bam_step}_flagstat.txt",
    log:
        config["data_dir"] + "/logs/alignment_qc_{library_id}_{bam_step}.err",
    shell:
        """
        samtools stats {input} > {output.samstat} 2>{log}
        samtools flagstat {input} > {output.flagstat} 2>>{log}
        """
#+end_src
*** DONE Sequencing depth metric(s)
#+begin_src snakemake
# Sequencing depth via Picard
rule picard_collect_wgs_metrics:
    input:
        config["data_dir"] + "/bam/{library_id}_dedup.bam",
    output:
        config["data_dir"] + "/qc/{library_id}_collect_wgs_metrics.txt",
    shell:
        """
        {config[cfdna_wgs_script_dir]}/CollectWgsMetrics_wrapper.sh {input} {config[genome_fasta]} {output}
        """
#+end_src
#+begin_src bash :tangle ./workflow/scripts/CollectWgsMetrics_wrapper.sh
picard CollectWgsMetrics \
       INPUT=$1 \
       OUTPUT=$3 \
       READ_LENGTH=150 \
       REFERENCE_SEQUENCE=$2

#+end_src

*** DONE Fragment sizes                                               :smk_rule:
#+begin_src snakemake
# Fragment sizes by deepTools
rule deeptools_bamprfragmentsize:
    input:
        config["data_dir"] + "/bam/{library_id}_dedup.bam",
    params:
        blacklist = config["blacklist"],
    output:
        config["data_dir"] + "/qc/{library_id}_deeptools_frag_lengths.txt",
    shell:
        """
        {config[cfdna_wgs_script_dir]}/bamPEFragmentSize_wrapper.sh \
        {input} \
        {config[threads]} \
        {params[blacklist]} \
        {output}
        """
#+end_src

#+begin_src bash :tangle ./workflow/scripts/bamPEFragmentSize_wrapper.sh
#!/usr/bin/env bash
input=$1
threads=$2
blacklist=$3
output=$4

bamPEFragmentSize --bamfiles $input \
                  --numberOfProcessors $threads \
                  --blackListFileName $blacklist \
                  --outRawFragmentLengths $output
#+end_src

  #+begin_src bash
#########1#########2#########3#########4#########5#########6#########7#########8
#
source ./src/setup.sh
docker_interactive
jeszyman
biotools
source ~/repos/mpnst/src/setup.sh

# Function
mpnst_fragsize() {
    bamPEFragmentSize --bamfiles $1 \
                      --numberOfProcessors $2 \
                      --binSize $3 \
                      --distanceBetweenBins $4 \
                      --outRawFragmentLengths $5
}

##
## Local variables
processors=40
bin_size=10000000
distance_between_bins=10000000
min_bam_size=100000000

#
# Generate bam file lists
#  Note: Small or empty bams kill bamPEFragmentSize and must be excluded
##
## For fragment-filtered bams
declare -a frag_filt_bam=()
for file in $localdata/frag-filt-bams/*.dedup.sorted.frag.sorted.bam;
do
    bamsize=$(wc -c <"$file")
    if [ $bamsize -ge $min_bam_size ]; then
        frag_filt_bam+=("$file")
    fi
done
##
## For deduped full bams
declare -a dedup_bam
for file in $localdata/bams/*.dedup.sorted.bam;
do
    bamsize=$(wc -c <"$file")
    if [ $bamsize -ge $min_bam_size ]; then
        dedup_bam+=("$file")
    fi
done
##
mkdir -p $localdata/frag_size
#
for file in "${frag_filt_bam[@]}";
do
    base=`basename $file`
    if [ $localdata/frag_size/${base}.fragsize.tsv -ot $file ]; then
        mpnst_fragsize \
            $file \
            $processors \
            $bin_size \
            $distance_between_bins \
            $localdata/frag_size/${base}.fragsize.tsv
    fi
done
#
for file in "${dedup_bam[@]}";
do
    base=`basename $file`
    if [ $localdata/frag_size/${base}.fragsize.tsv -ot $file ]; then
        mpnst_fragsize \
            $file \
            $processors \
            $bin_size \
            $distance_between_bins \
            $localdata/frag_size/${base}.fragsize.tsv
    fi
done
#
rm $localdata/frag_size/frag_size_summary.tsv
touch $localdata/frag_size/frag_size_summary.tsv
for file in $localdata/frag_size/*.fragsize.tsv; do
    cat $file | tail -n +3 >> $localdata/frag_size/frag_size_summary.tsv
done
#
sed -i '1 i\size\toccurences\tsample' $localdata/frag_size/frag_size_summary.tsv
#
rm $repo/data/frag_size_summary.tsv
rm $repo/data/frag_size_summary_too_big
#
summary_file_size=$(wc -c <"$localdata/frag_size/frag_size_summary.tsv")
max_size=1000000
if [ $summary_file_size -gt $max_size ]; then
    touch $repo/data/frag_size_summary_too_big
else
    cp $localdata/frag_size/frag_size_summary.tsv $repo/data/frag_size_summary.tsv
fi
#
exit
#+end_src
  - Fragment size
    #+name: fragment-sampling
    #+begin_src bash
  #
  # Samples fragment size by TLEN in bam files
  #
  # Setup
  exit
  source ~/repos/mpnst/bin/local-setup.sh
  ## Variables
  fragsampledir=$localdata/tmp
  ## Directories
  rm -rf $fragsampledir
  mkdir -p $fragsampledir
  #
  # Get lists of bam files to sample
  find /localdata/box/NCI FASTQ/ -name
  find /duo4/.mpnst/bam-nci/ -name "*.dedup.bam" > $fragsampledir/nci-invivo-bams
  find /duo4/.mpnst/bam-nci/ -name "*.filt.sorted.bam" > $fragsampledir/nci-insilico-bams
  #TODO ADD WASHU find /duo4/mpnst/

  # TODO
  ## paramaterize sampleing count
  #
  # Run Setup
  #
  # Processes
  ##
  #
  mapfile -t nci_insilico_bams < $fragsampledir/nci-insilico-bams
  for file in "${nci_insilico_bams[@]}"; do
      prebase=`basename $file`
      base="${prebase%%.*}"
      sambamba view -f sam -t 30 $file | shuf --head-count 10000 > $fragsampledir/${base}_nci_insilico_sample
  done
  #
  #########1#########2#########3#########4#########5#########6#########7#########8
  mapfile -t nci_invivo_bams < $fragsampledir/nci-invivo-bams
  for file in "${nci_invivo_bams[@]}"; do
      prebase=`basename $file`
      base="${prebase%%.*}"
      sambamba view -f sam -t 30 $file | shuf --head-count 10000 > $fragsampledir/${base}_nci_invivo_sample
  done

  cd $fragsampledir
  rm frag_concat.txt
  for file in $fragsampledir/*_sample; do
      awk '{ print sqrt($9^2) "_" FILENAME }' $file >> frag_concat.txt
  done
  sed -i '1s/^/fragsize_\n/' frag_concat.txt
  >>>>>>> 2d6bf2d62424a76f5893600fce7444a867784228

  sed -i -e 's/_/,/g' frag_sum_test.txt



  # find /duo4/.mpnst/fastq-washu/ -name "*HiSeqW31*R1_001_TAGC*.fastq.gz" | cut -d "_" -f 1-5
  #      | parallel perl ~/repos/mpnst/bin/cp-fastq-extract-auto.pl {}\_R1_001_TAGC.fastq.gz {}\_R2_001_TAGC.fastq.gz -j 24

  #+end_src
    #+begin_src bash
  source ./src/setup.sh
  docker_interactive
  jeszyman
  biotools
  source ~/repos/mpnst/src/setup.sh
  source ~/repos/mpnst/src/functions.sh

  for file in $dataDIR/bam/lib*_sub20m.bam;
  do
      base=$(basename -s .bam $file)
      if [ $file -nt $dataDIR/bam/${base}_frag90_150_sorted.bam ];
      then
          frag_filter $file \
                      $dataDIR/bam \
                      90 \
                      150 \
                      40
      fi
  done
  #+end_src
*** DONE Make QC table                                                :smk_rule:
#+begin_src snakemake
checkpoint make_qc_tbl:
    input:
        config["data_dir"] + "/qc/all_qc_data/multiqc_samtools_stats.txt",
    params:
        script = config["cfdna_wgs_script_dir"] + "/make_qc_tbl.R"
    output:
        config["data_dir"] + "/qc/read_qc.tsv",
    log:
        config["data_dir"] + "/logs/read_qc.log"
    shell:
        """
        Rscript {params.script} \
        {input} \
        {output} \
        >& {log}
        """
#+end_src
#+begin_src R :tangle ./workflow/scripts/make_qc_tbl.R
args = commandArgs(trailingOnly = TRUE)
samstats = args[1]
read_qc_tbl = args[2]

library(dplyr)

read_qc = as_tibble(read.table(samstats, header = TRUE, sep = '\t')) %>%
  filter(grepl("dedup", Sample)) %>%
  mutate(library_id = substr(Sample,1,6)) %>%
  mutate(dedup_reads_properly_paired = reads_properly_paired) %>%
  select(library_id, dedup_reads_properly_paired)

write.table(read_qc, file = read_qc_tbl, row.names=F, sep = '\t', quote = F)
#+end_src
*** TODO Use QC table as checkpoint
#+begin_src snakemake

def get_results(wildcards):
    read_qc = pd.read_table("test/qc/read_qc.tsv")
    test=read_qc.library_id[read_qc.dedup_reads_properly_paired > 2000].tolist()
    return expand(
	config["data_dir"] + "/bam/{library_id}_ds{milreads}.bam",
        library_id=test)

#+end_src
*** TODO Downsample bams                                           :smk_rule:
#+begin_src snakemake
# Alignment downsampling
rule downsample_bams:
    input:
        config["data_dir"] + "/bam/{library_id}_dedup.bam",
    output:
        config["data_dir"] + "/bam/{library_id}_ds{milreads}.bam",
    log:
        config["data_dir"] + "/logs/downsample_bam_{library_id}_{milreads}.err"
    shell:
        """
        {config[cfdna_wgs_script_dir]}/downsample_bam.sh {input} {wildcards.milreads}000000 {output} 2>{log}
        """
#+end_src

#+begin_src bash :tangle ./workflow/scripts/downsample_bam.sh
## Calculate the sampling factor based on the intended number of reads:
FACTOR=$(samtools idxstats $1 | cut -f3 | awk -v COUNT=$2 'BEGIN {total=0} {total += $1} END {print COUNT/total}')

if [[ $FACTOR > 1 ]]; then
    echo "DS reads exceeds total for $1"
    cp $1 $3
else
    sambamba view -s $FACTOR -f bam -l 5 $1 > $3
fi
#+end_src


** [[file:workflow/cfdna_wgs_int_test.smk][Integration testing]]                                                  :smk:
- snakefile
  #+begin_src snakemake :tangle ./workflow/int_test.smk
import pandas as pd

container: config["container"]

libraries = pd.read_table(config["data_dir"] + "/inputs/libraries.tsv")

LIBRARY_IDS = list(libraries.library.unique())

MILREADS = config["MILREADS"]

def get_results(wildcards):
    read_qc = pd.read_table(checkpoints.make_qc_tbl.get().output[0])
    test=read_qc.library_id[read_qc.dedup_reads_properly_paired > 2000].tolist()
    return expand(
	config["data_dir"] + "/bam/{library_id}_ds{milreads}.bam",
        library_id=test, milreads = MILREADS)


rule all:
    input:
        expand(config["data_dir"] + "/fastq/raw/{library_id}_{read}.fastq.gz", library_id = LIBRARY_IDS, read = ["R1", "R2"]),
        #expand(config["data_dir"] + "/fastq/processed/{library_id}_proc_{read}.fastq.gz", library_id = LIBRARY_IDS, read = ["R1","R2"]),
        #expand(config["data_dir"] + "/fastq/unpaired/{library_id}_unpr_R1.fastq.gz", library_id = LIBRARY_IDS, read = ["R1","R2"]),
        #expand(config["data_dir"] + "/bam/{library_id}.sam", library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/qc/{library_id}_{read}_fastqc.html", library_id = LIBRARY_IDS, read = ["R1","R2"]),
        #expand(config["data_dir"] + "/qc/{library_id}_proc_{read}_fastqc.html", library_id = LIBRARY_IDS, read = ["R1","R2"]),
        #expand(config["data_dir"] + "/bam/{library_id}_dedup.bam", library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/qc/{library_id}_collect_wgs_metrics.txt", library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/bam/{library_id}_dedup.bam.bai", library_id = LIBRARY_IDS),
        #expand(config["data_dir"] + "/qc/{library_id}_{bam_step}_samstats.txt", library_id = LIBRARY_IDS, bam_step= ["dedup","raw"]),
        #expand(config["data_dir"] + "/qc/{library_id}_{bam_step}_flagstat.txt", library_id = LIBRARY_IDS, bam_step =["dedup","raw"]),
        #config["data_dir"] + "/qc/read_qc.tsv",
        #expand(config["data_dir"] + "/bam/{library_id}_ds{milreads}.bam", library_id = LIBRARY_IDS, milreads = MILREADS),
        #config["data_dir"] + "/qc/all_qc.html",
        #expand(config["data_dir"] + "/qc/{library_id}_deeptools_frag_lengths.txt", library_id = LIBRARY_IDS),
        #get_results,

rule symlink:
    input:
        config["data_dir"] + "/inputs/{library_id}_{read}.fastq.gz",
    output:
        config["data_dir"] + "/fastq/raw/{library_id}_{read}.fastq.gz",
    log:
        config["data_dir"] + "/logs/{library_id}_{read}_symlink.log"
    shell:
        """
        ln --force --relative --symbolic {input} {output} 2>{log}
        """

include: "read_preprocess.smk"

rule multiqc:
    input:
        expand(config["data_dir"] + "/qc/{library_id}_{read}_fastqc.html", library_id = LIBRARY_IDS, read = ["R1","R2"]),
        expand(config["data_dir"] + "/qc/{library_id}_proc_{read}_fastqc.html", library_id = LIBRARY_IDS, read = ["R1","R2"]),
        expand(config["data_dir"] + "/qc/{library_id}_{bam_step}_samstats.txt", library_id = LIBRARY_IDS, bam_step= ["dedup","raw"]),
        expand(config["data_dir"] + "/qc/{library_id}_{bam_step}_flagstat.txt", library_id = LIBRARY_IDS, bam_step =["dedup","raw"]),
    params:
        out_dir = config["data_dir"] + "/qc"
    output:
        config["data_dir"] + "/qc/all_qc.html"
    shell:
        """
        multiqc {params.out_dir} \
        --force \
        --outdir {params.out_dir} \
        --filename all_qc
        """

#+end_src
- shell scripts
  #+begin_src bash :tangle ./tools/shell/rm_outputs.sh
#!/usr/bin/env bash

# Remove output directories
outputs_dirs=("fastq"
              "bam"
              "logs"
              "qc")

for dir in "${outputs_dirs[@]}"; do
    if [ -d test/${dir} ]; then \rm -rf test/${dir}; fi
done

#+end_src
  #+begin_src bash :tangle ./tools/shell/int_test.sh
#!/usr/bin/env bash
outputs_dirs=("fastq"
              "bam"
              "logs"
              "qc")

for dir in "${outputs_dirs[@]}"; do
    if [ -d test/${dir} ]; then \rm -rf test/${dir}; fi
done

../basecamp/src/smk_dry_run.sh config/int_test.yaml workflow/int_test.smk &&
../basecamp/src/smk_draw.sh config/int_test.yaml workflow/int_test.smk resources/int_test.pdf &&
../basecamp/src/smk_forced_run.sh config/int_test.yaml workflow/int_test.smk &&
echo "Integration testing passed, do you want to erase results files?" &&
select yn in "Yes" "No"; do
    case $yn in
        Yes ) for dir in "${outputs_dirs[@]}"; do \rm -rf test/$dir; done; break;;
        No ) exit;;
    esac
done

#+end_src
- [[file:resources/int_test.pdf]]

#+transclude: [[id:f9f7130c-4fb4-4e08-99fc-3cf242587134][test]] :only-contents



** README
:PROPERTIES:
:export_file_name: ./README.md
:export_options: toc:nil
:END:
*** Changlog
- [2022-05-31 Tue] - Conforms to current biotools best practices.
- [2022-04-29 Fri] - Moved multiqc to integration testing as inputs are dependent on final sample labels. Integration testing works per this commit.
** Dev
:PROPERTIES:
:header-args:snakemake: :tangle no
:END:
- Prioritized [2022-06-07 Tue]
  1. input params checks
     - add # # TODO setup via fastqc metrics check
       - # for read1 in $fastqdir/*_R1.fastq.gz; do
         #     base=`basename -s _R1.fastq.gz ${read1}`
         #     filesize=$(wc -c <"$bamdir/${base}.bam")
         #     if [ $minimum_bam_size -ge $filesize ]; then
         #         echo $base >> /drive3/users/jszymanski/repos/mpnst/data/small_bams
         #     fi
         # done
         # readarray -t small_bam < /drive3/users/jszymanski/repos/mpnst/data/small_bams
  2. Library QC Dataframe
     #+begin_src R
   library(tidyverse)

   flagstat_raw = as_tibble(read.table("/home/jeszyman/repos/mpnst-preprocessing/test/qc/all_qc_data/multiqc_samtools_flagstat.txt", header = T, sep = '\t'))

   flagstat_mod =
     flagstat_raw %>%
     mutate(library_id = substr(Sample, 1, 6)) %>%
     mutate(bam_type = gsub("_.*$","", gsub("^.......","",Sample))) %>%
     pivot_wider(names_from = bam_type, values_from = -c(library_id, bam_type), everything()) %>%
     select(library_id, everything(), -starts_with("Sample"))

   samstats_raw = as_tibble(read.table("/home/jeszyman/repos/mpnst-preprocessing/test/qc/all_qc_data/multiqc_samtools_stats.txt", header = T, sep = '\t'))

   samstats_mod =
     samstats_raw %>%
     mutate(library_id = substr(Sample, 1, 6)) %>%
     mutate(bam_type = gsub("_.*$","", gsub("^.......","",Sample))) %>%
     pivot_wider(names_from = bam_type, values_from = -c(library_id, bam_type), everything()) %>%
     select(library_id, everything(), -starts_with("Sample"))
   samstats_mod

   #+end_src
  3. Mem cap and performance workup
  4. deeptools integration to multiqc https://multiqc.info/docs/
  5. https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html see multiext
- filter to min file size && expected by manual spreadsheet
- fastqs too small (< 500 Mb)
  #+begin_src bash :results replace
find /mnt/ris/aadel/mpnst/inputs/cappseq-fastq -size -500M
#+end_src
- expand seq depth metrics
  - https://gatk.broadinstitute.org/hc/en-us/articles/360037226132-CollectWgsMetrics-Picard-
  - https://github.com/brentp/mosdepth
  - https://deeptools.readthedocs.io/en/develop/content/tools/bamCoverage.html#Output
  - using mosdepth
    #+name: mosdepth
    #+begin_src bash
  #########1#########2#########3#########4#########5#########6#########7#########8
  #
  ### mosdepth for WGS depth calc  ###
  #
  # Setup
  ##

  # Mosdepth per bam dir
  ##
  ## For deduped bams
  for file in $localdata/bams/*.dedup.sorted.bam; do
      mosdepth_mpnst $file $localdata/bam-qc/dedup 250000000
  done
  ##
  #
  # get simple tsv and send to repo

  for file in $localdata/bam-qc/dedup/lib*.regions.bed.gz; do
      base=`basename -s .dedup.sorted.regions.bed.gz $file`
      zcat $file | awk -v FS='\t' -v var=$base 'NR <=24 {print var,$1,$4}' >> $localdata/bam-qc/dedup/all_dedup_coverage
  done

  header=library_id\\tchr\\tmean_coverage
  sed -i "1 i$header" $localdata/bam-qc/dedup/all_dedup_coverage

  ## Local
  >>>>>>> 2d6bf2d62424a76f5893600fce7444a867784228
  source ~/repos/mpnst/bin/local-setup.sh
  docker_interactive
  biotools
  ##
  ## Functions
  ###
  ### Convert bams to wigs
  bam_to_wig() {
      printf "Variables are: 1=bam_file 2=bam_suffix 3=outdir\n"
          base=`basename -s ${2} $1`
          if [ $3/${base}.wig -ot $1 ]; then
              /opt/hmmcopy_utils/bin/readCounter --window 1000000 --quality 20 \
                                                 --chromosome "chr1,chr2,chr3,chr4,chr5,chr6,chr7,chr8,chr9,chr10,chr11,chr12,chr13,chr14,chr15,chr16,chr17,chr18,chr19,chr20,chr21,chr22,chrX,chrY" $1 > $3/${base}.wig
          fi
  }
  ###
  ### Run ichor for low TF
  ichor_lowfract() {
      base=`basename -s .wig $1`
      if [ $2/$base.RData -ot $1 ]; then
          Rscript /opt/ichorCNA/scripts/runIchorCNA.R \
                  --id $base \
                  --WIG $1 \
                  --gcWig /opt/ichorCNA/inst/extdata/gc_hg19_1000kb.wig \
                  --normal "c(0.95, 0.99, 0.995, 0.999)" \
                  --ploidy "c(2)" \
                  --maxCN 3 \
                  --estimateScPrevalence FALSE \
                  --scStates "c()" \
                  --outDir $2
      fi
  }
  ##
  ##
  mkdir -p $localdata/wigs
  mkdir -p $localdata/ichor
  #
  # Make wigs
  #
  #bam_to_wig /mnt/xt3/mpnst/frag-filt-bams/lib109.dedup.sorted.frag90_150.sorted.bam .dedup.sorted.frag90_150.sorted.bam $localdata/wigs
  ##
  for file in $localdata/frag-filt-bams/lib109*.bam; do
      bam_to_wig $file \
                 .dedup.sorted.frag.sorted.bam \
                 $localdata/wigs
  done

  ## For fraction-filtered WGS cfDNA
  for file in $localdata/frag-filt-bams/*.bam; do
      bam_to_wig $file \
                 .dedup.sorted.frag.sorted.bam \
                 $localdata/wigs
  done
  ##
  ## For tumor and leukocyte WGS libraries
  ### Make array of genomic library file paths
  genomic=($(cat /drive3/users/jszymanski/repos/mpnst/data/libraries.csv | grep -e tumor -e leukocyte | grep -v "wes" | awk -F, '{print $1}' | sed 's/"//g' | sed 's/$/.dedup.sorted.bam/g' | sed 's/^/\/mnt\/xt3\/mpnst\/bams\//g'))
  ###
  for file in ${genomic[@]}; do
      bam_to_wig $file \
                 .dedup.sorted.bam \
                 $localdata/wigs
  done
  #
  ##
  ## Send successful file list to repo
  rm /drive3/users/jszymanski/repos/mpnst/data/wigs.tsv
  for file in $localdata/wigs/*.wig;
  do
      base=`basename -s .wig $file`
      echo $base >> /drive3/users/jszymanski/repos/mpnst/data/wigs.tsv
  done
  #
  ##RESUME HERE
  # ichor
  ##
  for file in $localdata/wigs/lib109*.wig; do
      ichor_lowfract $file $localdata/ichor
  done


  header=library_id\\tchr\\tmean_coverage
  sed -i "1 i$header" $localdata/bam-qc/dedup/all_dedup_coverage

  max_file_size=5000000
  file_size=$(
      wc -c <"$localdata/bam-qc/dedup/all_dedup_coverage"
           )

  if [ $filesize -gt $max_file_size ]; then
      touch $repo/data/qc/all_dedup_coverage_too_big
  else
      cp $localdata/bam-qc/dedup/all_dedup_coverage $repo/qc/all_dedup_coverage.tsv
  fi
  #
  #+end_src
    - Cant calcualte depths off [[file:~/repos/mpnst/data/bam_qc_data/mqc_mosdepth-coverage-per-contig_1.txt]] , d/n allow values under 1
    - [ ] for coverage, should intersect down to autosomes
  - run and extract mosdepth
    mosdepthRAW = as_tibble(read.table(file.path(repo,"data/all_dedup_coverage.tsv"), header = T, sep = '\t', fill = TRUE))
** Reference
- [[id:271b4d5f-727e-496e-b835-8fe9f8655655][Bioinformatics project module]]
*** [[id:13120759-71db-497c-8ed3-1c58e47a7840][Biotools headline]]
